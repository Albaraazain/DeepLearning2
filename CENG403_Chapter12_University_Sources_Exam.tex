\documentclass[12pt]{article}
\usepackage{fancyhdr}
\usepackage{amsmath,amsfonts,enumerate}
\usepackage{color,graphicx}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}
\usetikzlibrary{arrows,positioning,shapes,calc,matrix}
\pagestyle{fancy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Course customization based on university sources
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\masunitnumber}{CENG 403}
\newcommand{\examdate}{January 2025}
\newcommand{\academicyear}{2024-2025}
\newcommand{\semester}{I}
\newcommand{\coursename}{Deep Learning - CNN Architectures \& RNN Introduction (University Sources)}
\newcommand{\numberofhours}{3}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CUSTOM SPACING COMMANDS FOR ANSWER SPACES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\answerspace}[1]{\vspace{#1}}
\newcommand{\questionspace}{\vspace{3cm}}        
\newcommand{\subquestionspace}{\vspace{2.5cm}}   
\newcommand{\shortanswer}{\vspace{2cm}}          
\newcommand{\mediumanswer}{\vspace{3cm}}         
\newcommand{\longanswer}{\vspace{4cm}}           
\newcommand{\journalspace}{\vspace{4.5cm}}       
\newcommand{\codespace}{\vspace{5cm}}            
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Header setup
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lhead{}
\rhead{}
\chead{{\bf MIDDLE EAST TECHNICAL UNIVERSITY}}
\lfoot{}
\rfoot{}
\cfoot{}
\begin{document}
\setlength{\headsep}{5truemm}
\setlength{\headheight}{14.5truemm}
\setlength{\voffset}{-0.45truein}
\renewcommand{\headrulewidth}{0.0pt}
\begin{center}
SEMESTER \semester\ EXAMINATION \academicyear
\end{center}
\begin{center}
{\bf \masunitnumber\ -- \coursename}
\end{center}
\vspace{20truemm}
\noindent \examdate\hspace{45truemm} TIME ALLOWED: \numberofhours\ HOURS
\vspace{19truemm}
\hrule
\vspace{19truemm}
\noindent\underline{INSTRUCTIONS TO CANDIDATES}
\vspace{8truemm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Instructions based on university standards
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate}
\item This examination paper contains {\bf SEVEN (7)} questions and comprises 
{\bf TEN (10)} printed pages.
\item Answer all questions. 
The marks for each question are indicated at the beginning of each question.
\item Answer each question beginning on a {\bf FRESH} page of the answer book.
\item This {\bf IS NOT an OPEN BOOK} exam.
\item Show all mathematical derivations clearly with proper notation.
\item For architectural diagrams, draw clear and labeled components.
\item Calculate all requested parameters and show intermediate steps.
\item Explain computational complexity where requested.
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% New page for questions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\lhead{}
\rhead{\masunitnumber}
\chead{}
\lfoot{}
\cfoot{\thepage}
\rfoot{}
\setlength{\footskip}{45pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EXAM QUESTIONS BASED ON UNIVERSITY SOURCES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Question 1. CNN Architectural Fundamentals and Calculations}\hfill (25 marks)\\
Based on D2L.ai and university CNN course materials covering computational aspects.

\begin{enumerate}[(a)]
    \item For a convolutional layer with the following specifications, calculate the output dimensions and number of parameters: \hfill (12 marks)
    \begin{itemize}
        \item Input: 224×224×3 RGB image
        \item 64 filters of size 7×7
        \item Stride: 2
        \item Padding: 3
        \item Bias terms included
    \end{itemize}
    
    Show all calculations including:
    \begin{itemize}
        \item Output height and width
        \item Total number of parameters
        \item Memory requirements for storing activations
    \end{itemize}
    
    \journalspace
    
    \item Explain the difference between "Valid Padding" and "Same Padding" in CNNs. For a 12×12 input with a 3×3 filter and stride 1: \hfill (8 marks)
    \begin{itemize}
        \item Calculate output size with valid padding
        \item Calculate padding needed for same padding
        \item Discuss trade-offs between the two approaches
    \end{itemize}
    
    \mediumanswer
    
    \item Compare parameter sharing in CNNs versus fully connected networks. For an image of size 256×256×3, calculate the number of parameters needed for: \hfill (5 marks)
    \begin{itemize}
        \item First layer as fully connected (to 512 units)
        \item First layer as convolutional (64 filters, 5×5)
        \item Explain the computational advantage
    \end{itemize}
    
    \mediumanswer
\end{enumerate}

\newpage
\paragraph{Question 2. ResNet Architecture and Skip Connections}\hfill (30 marks)\\
Based on university deep learning courses and D2L.ai educational content.

\begin{enumerate}[(a)]
    \item Explain the mathematical foundation of residual learning. Given a target function $H(x)$, derive why learning the residual mapping $F(x) = H(x) - x$ is easier than learning $H(x)$ directly. \hfill (10 marks)
    
    Include discussion of:
    \begin{itemize}
        \item Identity function learning difficulty
        \item Gradient flow advantages
        \item Why zero functions are easier to learn
    \end{itemize}
    
    \mediumanswer
    
    \item Design and draw a complete ResNet basic block showing: \hfill (12 marks)
    \begin{itemize}
        \item Two 3×3 convolutional layers
        \item Skip connection implementation
        \item Activation function placement
        \item Dimension matching considerations
    \end{itemize}
    
    Compare this with a bottleneck block design (1×1, 3×3, 1×1 structure).
    
    \begin{center}
    \begin{tikzpicture}[scale=0.8]
        % Space for ResNet block diagram
        \draw[dotted] (0,0) rectangle (12,8);
        \node at (6,7.5) {\textbf{Draw ResNet Basic Block}};
        \node at (6,0.5) {\textbf{Include: Conv layers, skip connections, activations}};
    \end{tikzpicture}
    \end{center}
    
    \shortanswer
    
    \item Analyze gradient flow in ResNet vs. vanilla deep networks. For a 50-layer network, explain mathematically why ResNet can avoid vanishing gradients. \hfill (8 marks)
    
    Include:
    \begin{itemize}
        \item Gradient computation through skip connections
        \item Comparison with traditional deep networks
        \item Why identity mappings preserve gradient magnitude
    \end{itemize}
    
    \mediumanswer
\end{enumerate}

\newpage
\paragraph{Question 3. DenseNet and Advanced CNN Architectures}\hfill (22 marks)\\
Based on modern CNN architecture research and educational materials.

\begin{enumerate}[(a)]
    \item Compare DenseNet with ResNet architectures. Explain the key difference: \hfill (8 marks)
    $$\text{ResNet: } x_l = H_l(x_{l-1}) + x_{l-1}$$
    $$\text{DenseNet: } x_l = H_l([x_0, x_1, \ldots, x_{l-1}])$$
    
    Discuss advantages and disadvantages of each approach.
    
    \mediumanswer
    
    \item For a DenseNet block with 4 layers, each producing 12 feature maps (growth rate k=12), and input of 64 channels: \hfill (10 marks)
    \begin{itemize}
        \item Calculate the number of input channels for each layer
        \item Compute total memory requirements for concatenations
        \item Explain how transition layers reduce dimensionality
        \item Calculate parameters for 1×1 conv in transition layer
    \end{itemize}
    
    \journalspace
    
    \item Design a Highway Network gate mechanism. Write the mathematical equations for: \hfill (4 marks)
    $$y = H(x,W_H) \cdot T(x,W_T) + x \cdot C(x,W_C)$$
    
    Explain how this differs from standard residual connections.
    
    \shortanswer
\end{enumerate}

\newpage
\paragraph{Question 4. CNN Optimization and Efficiency}\hfill (20 marks)\\
Based on practical CNN implementation and optimization techniques.

\begin{enumerate}[(a)]
    \item Analyze binary neural networks for edge deployment. Given a standard CNN with: \hfill (10 marks)
    \begin{itemize}
        \item 10M parameters (32-bit floats)
        \item 50 GFLOPS for inference
    \end{itemize}
    
    Calculate:
    \begin{itemize}
        \item Memory reduction with binary weights
        \item Speed improvement estimates
        \item Accuracy trade-offs to consider
        \item When binary networks are appropriate
    \end{itemize}
    
    \mediumanswer
    
    \item Compare different normalization strategies in deep CNNs: \hfill (10 marks)
    \begin{itemize}
        \item Batch Normalization: benefits and limitations
        \item Why BatchNorm helps in very deep networks (10,000+ layers)
        \item Relationship between BatchNorm and gradient stability
        \item Alternative normalization methods
    \end{itemize}
    
    \mediumanswer
\end{enumerate}

\newpage
\paragraph{Question 5. RNN Fundamentals and Unfolding}\hfill (28 marks)\\
Based on sequence modeling and RNN theory from university courses.

\begin{enumerate}[(a)]
    \item Classify the following problems and suggest appropriate architectures: \hfill (8 marks)
    \begin{itemize}
        \item Image captioning
        \item Spam email detection
        \item Machine translation
        \item Real-time speech recognition
    \end{itemize}
    
    For each, specify: one-to-one, one-to-many, many-to-one, or many-to-many architecture.
    
    \mediumanswer
    
    \item Explain RNN unfolding process. For the recurrent equation: \hfill (12 marks)
    $$h_t = \tanh(W_{hx} x_t + W_{hh} h_{t-1} + b_h)$$
    $$y_t = W_{hy} h_t + b_y$$
    
    Draw the unfolded network for T=3 time steps showing:
    \begin{itemize}
        \item Weight sharing across time
        \item Hidden state connections
        \item How this becomes a feedforward network
        \item Why sequences of different lengths can be handled
    \end{itemize}
    
    \journalspace
    
    \item Discuss the Turing completeness of RNNs. Explain: \hfill (8 marks)
    \begin{itemize}
        \item What it means for RNNs to be Turing complete
        \item The role of recurrent connections in providing memory
        \item Difference between theoretical capacity and practical training
        \item Comparison with multilayer perceptrons as universal approximators
    \end{itemize}
    
    \mediumanswer
\end{enumerate}

\newpage
\paragraph{Question 6. RNN Training and Gradient Issues}\hfill (25 marks)\\
Based on RNN training theory and backpropagation through time.

\begin{enumerate}[(a)]
    \item Explain why hyperbolic tangent is preferred over ReLU in vanilla RNNs. Discuss: \hfill (8 marks)
    \begin{itemize}
        \item Need for bounded hidden state values
        \item Consistency of state representation across time
        \item Problems with unbounded activations in recurrent connections
        \item Trade-offs with gradient flow
    \end{itemize}
    
    \mediumanswer
    
    \item For an RNN unfolded for 100 time steps, analyze the vanishing gradient problem: \hfill (12 marks)
    \begin{itemize}
        \item Why this creates a 100-layer feedforward network
        \item Mathematical explanation of gradient diminishing
        \item Effect of squashing activation functions
        \item Impact on learning long-term dependencies
    \end{itemize}
    
    Include analysis of gradient computation:
    $$\frac{\partial L}{\partial W_{hh}} = \sum_{t=1}^T \frac{\partial L_t}{\partial h_t} \frac{\partial h_t}{\partial W_{hh}}$$
    
    \journalspace
    
    \item Compare solutions to RNN gradient problems: \hfill (5 marks)
    \begin{itemize}
        \item Gradient clipping for exploding gradients
        \item Penalty terms for vanishing gradients
        \item When each approach is appropriate
    \end{itemize}
    
    \shortanswer
\end{enumerate}

\newpage
\paragraph{Question 7. LSTM Architecture and Memory Mechanisms}\hfill (30 marks)\\
Based on LSTM theory and gating mechanisms for sequence modeling.

\begin{enumerate}[(a)]
    \item Design the complete LSTM architecture with mathematical equations. For input $x_t$, previous hidden state $h_{t-1}$, and previous cell state $C_{t-1}$, derive: \hfill (15 marks)
    
    \begin{itemize}
        \item Forget gate: $f_t = ?$
        \item Input gate: $i_t = ?$ 
        \item Candidate values: $\tilde{C}_t = ?$
        \item Cell state update: $C_t = ?$
        \item Output gate: $o_t = ?$
        \item Hidden state: $h_t = ?$
    \end{itemize}
    
    \journalspace
    
    \item Explain why LSTM solves the vanishing gradient problem. Focus on: \hfill (10 marks)
    \begin{itemize}
        \item Gradient flow through the cell state path
        \item Why $\frac{\partial C_t}{\partial C_{t-1}}$ doesn't involve squashing functions
        \item How this enables learning of long-term dependencies
        \item Mathematical comparison with vanilla RNN gradient flow
    \end{itemize}
    
    \mediumanswer
    
    \item Compare LSTM variants: \hfill (5 marks)
    \begin{itemize}
        \item LSTM with peephole connections
        \item Coupled forget and input gates
        \item GRU vs LSTM trade-offs
        \item When to choose each variant
    \end{itemize}
    
    \mediumanswer
\end{enumerate}

\vfill
\begin{center}{\bf END OF PAPER}\end{center>
\end{document>