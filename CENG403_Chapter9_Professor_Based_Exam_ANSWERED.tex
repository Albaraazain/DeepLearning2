\documentclass[12pt]{article}
\usepackage{fancyhdr}
\usepackage{amsmath,amsfonts,enumerate}
\usepackage{color}
\usepackage{xcolor}
\pagestyle{fancy}

% Define colors for answers
\definecolor{answercolor}{RGB}{0,100,0}
\definecolor{explanationcolor}{RGB}{0,0,139}

% Custom command for answers
\newcommand{\answer}[1]{{\color{answercolor}\textbf{Answer:} #1}}
\newcommand{\explanation}[1]{{\color{explanationcolor}#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Course customization based on Professor's teaching
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\masunitnumber}{CENG 403}
\newcommand{\examdate}{January 2025}
\newcommand{\academicyear}{2024-2025}
\newcommand{\semester}{I}
\newcommand{\coursename}{Deep Learning - CNN Fundamentals \& Convolution Types (ANSWERED)}
\newcommand{\numberofhours}{3}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Header setup
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lhead{}
\rhead{}
\chead{{\bf MIDDLE EAST TECHNICAL UNIVERSITY}}
\lfoot{}
\rfoot{}
\cfoot{}
\begin{document}
\setlength{\headsep}{5truemm}
\setlength{\headheight}{14.5truemm}
\setlength{\voffset}{-0.45truein}
\renewcommand{\headrulewidth}{0.0pt}
\begin{center}
SEMESTER \semester\ EXAMINATION \academicyear
\end{center}
\begin{center}
{\bf \masunitnumber\ -- \coursename}
\end{center}
\vspace{20truemm}
\noindent \examdate\hspace{45truemm} TIME ALLOWED: \numberofhours\ HOURS
\vspace{19truemm}
\hrule
\vspace{19truemm}
\noindent\underline{INSTRUCTIONS}
\vspace{8truemm}
\begin{enumerate}
\item This is the ANSWERED version with detailed explanations.
\item Each answer includes step-by-step reasoning to help you understand the concepts.
\item Pay attention to the connections between different concepts.
\item Focus on understanding WHY things work the way they do, not just memorizing.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% New page for questions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\lhead{}
\rhead{\masunitnumber}
\chead{}
\lfoot{}
\cfoot{\thepage}
\rfoot{}
\setlength{\footskip}{45pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EXAM QUESTIONS WITH DETAILED ANSWERS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Question 1. CNN Motivation and Limitations of Multi-Layer Perceptrons}{\hfill (25 marks)}\\
Based on the professor's discussion: "If you want to work with real life large scale problems let's say object recognition with high resolution...you want to recognize objects in such high resolution images what we do is we vectorize...then we use multiperceptron."

\begin{enumerate}[(a)]
    \item The professor calculated that for a 1000×1000 input image with 10,000 neurons in the first hidden layer, "just this layer is going to introduce 10 billion parameters." Explain why this parameter explosion is problematic and analyze the professor's statement that "more parameters means more data" with the quadratic relationship. \hfill (8 marks)
    
    \answer{
    Let me break down why 10 billion parameters is a massive problem:
    
    \textbf{The Calculation:}
    \begin{itemize}
        \item Input size: 1000 × 1000 pixels = 1,000,000 input values
        \item Hidden layer: 10,000 neurons
        \item Each neuron connects to EVERY input pixel
        \item Parameters = 1,000,000 × 10,000 = 10,000,000,000 (10 billion)
        \item Plus 10,000 bias terms (negligible compared to weights)
    \end{itemize}
    
    \textbf{Why This Is Problematic:}
    
    1. \textbf{Memory Requirements:}
    \explanation{
    If we store each parameter as a 32-bit float (4 bytes), we need:
    10 billion × 4 bytes = 40 GB just for ONE layer's parameters!
    During training, we also need to store gradients, activations, and optimizer states, easily exceeding 100GB for this single layer.
    }
    
    2. \textbf{Data Requirements - The Quadratic Relationship:}
    \explanation{
    The professor's key insight: as parameters increase, data requirements grow even faster.
    \begin{itemize}
        \item With 10 billion parameters, we risk severe overfitting
        \item Rule of thumb: need at least 10× more training examples than parameters
        \item This means we'd need 100+ billion training examples!
        \item The relationship is quadratic because: doubling parameters typically requires 4× more data to maintain the same generalization performance
    \end{itemize}
    }
    
    3. \textbf{Computational Cost:}
    \explanation{
    Each forward pass requires 10 billion multiply-add operations just for this layer.
    Training involves millions of forward and backward passes.
    This makes training prohibitively expensive and slow.
    }
    
    4. \textbf{Overfitting Risk:}
    \explanation{
    With so many parameters, the network can memorize the entire training set rather than learning general patterns.
    Each parameter is a "degree of freedom" - more parameters mean the model can fit increasingly complex (and potentially spurious) patterns.
    }
    }
    
    \item Analyze the professor's examples of equivariance and invariance problems. For image segmentation, explain why we need "if you transform the input...we expect the output to be transformed by the same transformation." For object recognition, explain why we need the prediction to "be independent of that transformation." \hfill (10 marks)
    
    \answer{
    Let me explain these two crucial but different requirements:
    
    \textbf{Equivariance for Image Segmentation:}
    
    \explanation{
    Image segmentation assigns a label to EACH PIXEL (e.g., "car", "road", "sky").
    
    Example scenario:
    \begin{itemize}
        \item Original image: Car in the center
        \item Segmentation output: Pixels labeled as "car" in the center
        \item Now shift the image 10 pixels right
        \item Expected: The "car" labels should ALSO shift 10 pixels right
    \end{itemize}
    
    Why this matters:
    \begin{itemize}
        \item The spatial relationship between input and output must be preserved
        \item If a pixel at position (100, 200) is labeled "car", and we shift the image by (10, 20), that same car pixel should now be labeled at position (110, 220)
        \item This is called EQUIVARIANCE: the output transforms in the same way as the input
    \end{itemize}
    
    Mathematical notation: If T is a transformation, we want: f(T(x)) = T(f(x))
    }
    
    \textbf{Invariance for Object Recognition:}
    
    \explanation{
    Object recognition produces a SINGLE label for the entire image (e.g., "this image contains a cat").
    
    Example scenario:
    \begin{itemize}
        \item Original image: Cat in top-left corner → Output: "cat"
        \item Shifted image: Cat in bottom-right corner → Output: still "cat"
        \item The location doesn't matter!
    \end{itemize}
    
    Why this matters:
    \begin{itemize}
        \item We want to recognize objects regardless of their position
        \item A cat is a cat whether it's in the center, corner, or anywhere else
        \item This is called INVARIANCE: the output stays the same despite input transformations
    \end{itemize}
    
    Mathematical notation: We want: f(T(x)) = f(x)
    }
    
    \textbf{The Key Difference:}
    \explanation{
    \begin{itemize}
        \item Segmentation: Spatial correspondence matters (equivariance)
        \item Recognition: Only content matters, not location (invariance)
        \item CNNs naturally provide equivariance through convolution
        \item CNNs achieve invariance through pooling layers that aggregate spatial information
    \end{itemize}
    }
    }
    
    \item The professor showed how shifting an image by one pixel in a multi-layer perceptron causes "all of the activations...they are going to change." Explain why this is a "very severe limitation" and how CNNs address this through architectural design. \hfill (7 marks)
    
    \answer{
    \textbf{The MLP Problem with Pixel Shifts:}
    
    \explanation{
    In an MLP, each neuron has a unique weight for each pixel position:
    \begin{itemize}
        \item Neuron 1 might have weight $w_{1,1}$ for pixel (0,0), $w_{1,2}$ for pixel (0,1), etc.
        \item When we shift the image by one pixel, EVERY pixel moves to a new position
        \item Pixel that was at (0,0) is now at (0,1)
        \item But the weights don't shift! Weight $w_{1,1}$ still looks at position (0,0)
        \item Result: Every neuron sees completely different input values
    \end{itemize}
    }
    
    \textbf{Why This Is a Severe Limitation:}
    
    \explanation{
    1. \textbf{No Spatial Understanding:}
    The network learns "there should be an edge at pixel (100,200)" instead of "edges are important features wherever they appear"
    
    2. \textbf{Poor Generalization:}
    A cat learned in the top-left won't be recognized in the bottom-right
    Each position needs to be learned separately, multiplying training data requirements
    
    3. \textbf{Wasted Parameters:}
    We need different weights to detect the same pattern at each possible location
    }
    
    \textbf{How CNNs Solve This:}
    
    \explanation{
    1. \textbf{Shared Weights (Convolution Filters):}
    \begin{itemize}
        \item Same 3×3 filter slides across the entire image
        \item The filter that detects edges works the same way at EVERY position
        \item When image shifts, the same filter just detects the pattern at the new location
    \end{itemize}
    
    2. \textbf{Local Connectivity:}
    \begin{itemize}
        \item Each neuron only looks at a small region (e.g., 3×3 pixels)
        \item Shifting by one pixel means most of the receptive field content stays similar
        \item Only the edges of the receptive field change
    \end{itemize}
    
    3. \textbf{Translation Equivariance:}
    \begin{itemize}
        \item If input shifts right, the activation pattern also shifts right
        \item The network's response moves with the content
        \item This is exactly what we want for robust vision systems
    \end{itemize}
    }
    }
\end{enumerate}

\newpage
\paragraph{Question 2. Neuroscience Inspiration and CNN Fundamentals}{\hfill (22 marks)}\\
The professor discussed Hubel and Wiesel's Nobel Prize-winning findings: "They did recordings real recordings from the brains of cats...they showed that...connectivity in biological neurons are not fully connected...neurons have receptive fields."

\begin{enumerate}[(a)]
    \item Explain the two key findings from Hubel and Wiesel that inspired CNN design: restricted connectivity and columnar structure. Describe how these translate to "receptive fields" and specialized neurons for "recognizing certain patterns at different scales." \hfill (10 marks)
    
    \answer{
    \textbf{Hubel and Wiesel's Groundbreaking Discoveries:}
    
    \explanation{
    They recorded electrical signals from individual neurons in cat visual cortex while showing various visual stimuli. This revealed how biological vision actually works.
    }
    
    \textbf{Finding 1: Restricted Connectivity (Receptive Fields)}
    
    \explanation{
    \textbf{What they found:}
    \begin{itemize}
        \item Each neuron in visual cortex responds only to stimuli in a SMALL region of the visual field
        \item This region is called the neuron's "receptive field"
        \item A neuron might only "see" a 5° × 5° patch of the visual world
        \item Different neurons have receptive fields in different locations
    \end{itemize}
    
    \textbf{How this translates to CNNs:}
    \begin{itemize}
        \item Convolutional neurons have small receptive fields (e.g., 3×3 or 5×5 pixels)
        \item Each neuron only connects to a local patch, not the entire image
        \item This dramatically reduces parameters: instead of connecting to all 1 million pixels, connect to just 9 (for 3×3)
        \item Different positions in the feature map correspond to different receptive field locations
    \end{itemize}
    }
    
    \textbf{Finding 2: Columnar Structure (Feature Specialization)}
    
    \explanation{
    \textbf{What they found:}
    \begin{itemize}
        \item Neurons are organized in columns perpendicular to cortex surface
        \item Within a column, neurons respond to the SAME location but DIFFERENT features
        \item Simple cells: Respond to edges at specific orientations (0°, 45°, 90°, etc.)
        \item Complex cells: Respond to edges regardless of exact position within receptive field
        \item Hypercomplex cells: Respond to more complex patterns like corners or line endings
    \end{itemize}
    
    \textbf{How this translates to CNNs:}
    \begin{itemize}
        \item Multiple filters (channels) at each layer = different neurons in a column
        \item Each filter learns to detect different patterns
        \item Early layers: Simple patterns (edges, colors)
        \item Deeper layers: Complex patterns (textures, parts, objects)
        \item Hierarchical feature learning mimics simple → complex → hypercomplex progression
    \end{itemize}
    }
    
    \textbf{Pattern Recognition at Different Scales:}
    
    \explanation{
    \begin{itemize}
        \item In biology: Neurons deeper in visual pathway have larger receptive fields
        \item In CNNs: Deeper layers see larger image regions due to stacked convolutions
        \item Layer 1: 3×3 receptive field (detects edges)
        \item Layer 2: 5×5 effective receptive field (detects corners, simple textures)
        \item Layer 5: 51×51 effective receptive field (detects entire objects)
        \item This allows recognizing patterns from local edges to global objects
    \end{itemize}
    }
    }
    
    \item Compare the evolution from Neocognitron (1979) to CNNs. Explain how Fukushima's "simple cells" and "complex cells" correspond to "convolution" and "pooling" in modern CNNs, and why gradient descent training was crucial for practical success. \hfill (8 marks)
    
    \answer{
    \textbf{Neocognitron (1979) - Fukushima's Architecture:}
    
    \explanation{
    Fukushima directly implemented Hubel and Wiesel's findings in an artificial network:
    }
    
    \textbf{S-cells (Simple Cells) → Modern Convolution Layers:}
    \explanation{
    \begin{itemize}
        \item \textbf{S-cells:} Detected specific patterns at specific locations
        \item Fixed, hand-designed templates (not learned)
        \item Responded strongly to exact pattern matches
        \item \textbf{Modern Convolution:} Learnable filters that detect patterns
        \item Filters are learned through backpropagation
        \item Can detect subtle variations of patterns
    \end{itemize}
    
    Key similarity: Both perform template matching with spatial specificity
    Key difference: Convolution filters are learned, not hand-designed
    }
    
    \textbf{C-cells (Complex Cells) → Modern Pooling Layers:}
    \explanation{
    \begin{itemize}
        \item \textbf{C-cells:} Provided position tolerance within local regions
        \item Responded if pattern appeared anywhere in a small area
        \item Created invariance to small shifts
        \item \textbf{Modern Pooling:} Max or average over local regions
        \item Reduces spatial resolution while keeping important features
        \item Provides translation invariance and computational efficiency
    \end{itemize}
    
    Key similarity: Both aggregate local information to create position tolerance
    Key difference: Pooling is simpler (just max/average) vs complex cell computations
    }
    
    \textbf{Why Gradient Descent Was Crucial:}
    
    \explanation{
    1. \textbf{Learning vs Hand-Design:}
    \begin{itemize}
        \item Neocognitron required manually designing each S-cell template
        \item Impossible to hand-design thousands of filters for complex tasks
        \item Gradient descent automatically learns optimal filters from data
    \end{itemize}
    
    2. \textbf{End-to-End Optimization:}
    \begin{itemize}
        \item Neocognitron trained layer-by-layer with unsupervised rules
        \item No guarantee that features useful for final task
        \item Backpropagation optimizes all layers jointly for the specific task
    \end{itemize}
    
    3. \textbf{Scalability:}
    \begin{itemize}
        \item Gradient descent scales to millions of parameters
        \item Can learn hierarchical representations automatically
        \item Enables training on massive datasets (ImageNet: 1.2M images)
    \end{itemize}
    
    4. \textbf{Performance Gap:}
    \begin{itemize}
        \item Neocognitron: Limited to simple digit recognition
        \item Modern CNNs: State-of-the-art on complex real-world vision tasks
        \item The difference: learned features far exceed hand-designed ones
    \end{itemize}
    }
    }
    
    \item The professor mentioned that in CNNs we use "fixed receptive field size for every neuron" unlike the brain where "in the central parts...we have smaller receptive field...in the peripheries...we have higher receptive field." Discuss the implications of this design choice. \hfill (4 marks)
    
    \answer{
    \textbf{Biological Vision - Variable Receptive Fields:}
    
    \explanation{
    The human visual system has:
    \begin{itemize}
        \item \textbf{Fovea (center):} Tiny receptive fields, high resolution
        \item Used for detailed tasks like reading
        \item \textbf{Periphery:} Large receptive fields, low resolution  
        \item Used for motion detection, general awareness
        \item This matches how we use vision: focus on details at center, monitor surroundings
    \end{itemize}
    }
    
    \textbf{CNNs - Fixed Receptive Fields:}
    
    \explanation{
    All neurons in a CNN layer have identical receptive field sizes (e.g., all use 3×3).
    }
    
    \textbf{Implications of Fixed Size:}
    
    \explanation{
    \textbf{Advantages:}
    \begin{itemize}
        \item \textbf{Computational Efficiency:} Uniform operations enable GPU optimization
        \item \textbf{Parameter Sharing:} Same weights used everywhere reduces memory
        \item \textbf{Simplicity:} Easier to implement and reason about
        \item \textbf{Translation Equivariance:} Consistent behavior across image
    \end{itemize}
    
    \textbf{Disadvantages:}
    \begin{itemize}
        \item \textbf{Inefficiency:} Same computation everywhere, even in "boring" regions
        \item \textbf{No Attention:} Can't focus more resources on important areas
        \item \textbf{Scale Limitations:} Fixed size may miss multi-scale patterns
    \end{itemize}
    
    \textbf{Modern Solutions:}
    \begin{itemize}
        \item \textbf{Attention Mechanisms:} Dynamically weight different regions
        \item \textbf{Deformable Convolutions:} Adaptive receptive fields
        \item \textbf{Multi-Scale Architectures:} Different branches for different scales
        \item These bring CNNs closer to biological flexibility while maintaining efficiency
    \end{itemize}
    }
    }
\end{enumerate}

\newpage
\paragraph{Question 3. CNN Architecture and Parameter Sharing}{\hfill (20 marks)}\\
The professor emphasized: "The first critical bit is that connectivity is restricted...the second important change...is that the weights are shared...we have the same parameters shared and used by every receptive field."

\begin{enumerate}[(a)]
    \item Explain how restricted connectivity and parameter sharing address the dimensionality problem. The professor stated: "if this is 1 million...the number of parameters here it doesn't depend on that actually...we just have three parameters W1 W2 W3." \hfill (8 marks)
    
    \answer{
    \textbf{Understanding the Dimensionality Problem:}
    
    \explanation{
    First, let's see what happens WITHOUT these two innovations:
    \begin{itemize}
        \item Input: 1 million pixels (1000×1000 image)
        \item Hidden layer: 10,000 neurons
        \item Full connectivity: 1M × 10K = 10 billion parameters
    \end{itemize}
    }
    
    \textbf{Solution 1: Restricted Connectivity}
    
    \explanation{
    Instead of connecting to ALL pixels, each neuron connects to a SMALL LOCAL REGION:
    \begin{itemize}
        \item Example: 3×3 receptive field = 9 connections per neuron
        \item Even with 10,000 neurons: 9 × 10,000 = 90,000 parameters
        \item Reduction: From 10 billion to 90,000 (99.999\% fewer!)
    \end{itemize}
    
    But wait, this alone isn't enough...
    }
    
    \textbf{Solution 2: Parameter Sharing}
    
    \explanation{
    The KEY INSIGHT: Use the SAME weights for every spatial location!
    
    The professor's example with W1, W2, W3:
    \begin{itemize}
        \item Imagine a 1D convolution with 3 weights: [W1, W2, W3]
        \item Position 0: Uses W1 for pixel 0, W2 for pixel 1, W3 for pixel 2
        \item Position 1: Uses W1 for pixel 1, W2 for pixel 2, W3 for pixel 3
        \item Position 2: Uses W1 for pixel 2, W2 for pixel 3, W3 for pixel 4
        \item And so on...
    \end{itemize}
    
    The SAME three weights slide across the entire input!
    }
    
    \textbf{The Magic: Parameters Don't Depend on Input Size!}
    
    \explanation{
    For a 2D image with 3×3 filters:
    \begin{itemize}
        \item 100×100 image: 9 parameters per filter
        \item 1000×1000 image: STILL 9 parameters per filter
        \item 10,000×10,000 image: STILL JUST 9 parameters per filter!
    \end{itemize}
    
    This is what the professor means by "doesn't depend on that actually"!
    
    Total parameters = (filter size) × (input channels) × (output channels)
    NOT multiplied by image dimensions!
    }
    
    \textbf{Complete Example:}
    \explanation{
    \begin{itemize}
        \item Input: 1000×1000×3 (RGB image)
        \item Layer 1: 64 filters of size 3×3
        \item Parameters: 3×3×3×64 = 1,728 parameters
        \item Compare to fully connected: 3,000,000×64 = 192 million parameters
        \item Reduction factor: 111,111× fewer parameters!
    \end{itemize}
    }
    }
    
    \item Analyze the trade-off the professor discussed: "if you restrict connectivity...a neuron receives information just from a restricted part of the input" versus the solution of increasing depth so "neurons in the following layers...will have the chance to integrate information across the whole input." \hfill (8 marks)
    
    \answer{
    \textbf{The Fundamental Trade-off:}
    
    \explanation{
    When we restrict connectivity, we create a limitation: each neuron has "tunnel vision" - it can only see a tiny part of the image.
    }
    
    \textbf{The Problem with Restricted Connectivity:}
    
    \explanation{
    \textbf{Layer 1 Neuron Limitations:}
    \begin{itemize}
        \item Can only see 3×3 pixels
        \item Cannot detect patterns larger than 3×3
        \item Cannot understand relationships between distant image regions
        \item Like looking at an elephant through a straw!
    \end{itemize}
    
    \textbf{Example:} 
    To recognize a face, you need to see eyes, nose, mouth, and their spatial relationships. A 3×3 receptive field can't even see one complete eye!
    }
    
    \textbf{The Solution: Depth (Stacking Layers)}
    
    \explanation{
    The professor's insight: Use multiple layers to gradually increase receptive field!
    
    \textbf{How Receptive Fields Grow:}
    \begin{itemize}
        \item Layer 1: Each neuron sees 3×3 pixels
        \item Layer 2: Each neuron sees 3×3 neurons from Layer 1
        \item But each Layer 1 neuron already sees 3×3 pixels
        \item So Layer 2 effectively sees 5×5 pixels
        \item Layer 3: Effectively sees 7×7 pixels
        \item Layer 4: Effectively sees 9×9 pixels
        \item And so on...
    \end{itemize}
    
    Formula: Effective receptive field = 1 + (k-1) × L
    Where k = kernel size, L = layer number
    }
    
    \textbf{Information Integration Process:}
    
    \explanation{
    \textbf{Layer-by-Layer Integration:}
    \begin{itemize}
        \item \textbf{Early layers:} Detect local features (edges, colors, textures)
        \item Small receptive fields are GOOD here - we want precise localization
        \item \textbf{Middle layers:} Combine local features into parts (eyes, wheels, corners)
        \item Medium receptive fields integrate local patterns
        \item \textbf{Deep layers:} Recognize complete objects (faces, cars, buildings)
        \item Large receptive fields see enough context for recognition
    \end{itemize}
    }
    
    \textbf{The Beautiful Balance:}
    
    \explanation{
    This design achieves multiple goals simultaneously:
    
    1. \textbf{Efficiency:} Few parameters per layer (due to small filters)
    2. \textbf{Expressiveness:} Can learn complex hierarchical features
    3. \textbf{Global Understanding:} Deep layers see the entire image
    4. \textbf{Local Precision:} Early layers maintain spatial accuracy
    
    It's like building understanding: you read letters → words → sentences → meaning!
    }
    
    \textbf{Practical Implications:}
    \explanation{
    \begin{itemize}
        \item Shallow networks with large filters: Many parameters, less expressive
        \item Deep networks with small filters: Fewer parameters, more expressive
        \item This is why modern networks (ResNet, EfficientNet) are deep, not wide
        \item VGGNet proved this: only 3×3 filters, but 16-19 layers deep
    \end{itemize}
    }
    }
    
    \item The professor showed that parameter sharing provides "equivariance to translation...if you shifted the input...the pattern will shift to the next receptive field...accordingly the activations will shift as well." Explain this mathematical property and why it doesn't extend to scale and rotation. \hfill (4 marks)
    
    \answer{
    \textbf{Translation Equivariance - Mathematical Definition:}
    
    \explanation{
    A function f is equivariant to transformation T if:
    \[f(T(x)) = T(f(x))\]
    
    For translation by vector $\vec{v}$:
    \[f(\text{translate}(x, \vec{v})) = \text{translate}(f(x), \vec{v})\]
    
    In simple terms: "If you move the input, the output moves the same way"
    }
    
    \textbf{Why Convolution IS Translation Equivariant:}
    
    \explanation{
    Example with 1D convolution:
    \begin{itemize}
        \item Input: [0, 0, 1, 2, 3, 0, 0]
        \item Filter: [1, -1] (edge detector)
        \item Output: [0, 1, 1, 1, -3, 0]
        \item Now shift input right by 2: [0, 0, 0, 0, 1, 2, 3]
        \item New output: [0, 0, 0, 1, 1, 1, -3]
        \item The output pattern shifted right by 2 as well!
    \end{itemize}
    
    This works because the SAME filter slides across all positions.
    }
    
    \textbf{Why Convolution is NOT Scale Equivariant:}
    
    \explanation{
    \begin{itemize}
        \item 3×3 filter detects edges at specific scale
        \item If you zoom in 2×, edges become 2× thicker
        \item Same 3×3 filter now sees different patterns
        \item A vertical edge might now look like a gradient to the small filter
        \item Output changes completely, not just scaled
    \end{itemize}
    
    Solution: Multi-scale architectures (pyramid networks) process multiple scales
    }
    
    \textbf{Why Convolution is NOT Rotation Equivariant:}
    
    \explanation{
    \begin{itemize}
        \item Filter learned to detect vertical edges: [[1, 0, -1], [1, 0, -1], [1, 0, -1]]
        \item Rotate image 90°: vertical edges become horizontal
        \item Same filter now produces zero response!
        \item Would need different filter [[1, 1, 1], [0, 0, 0], [-1, -1, -1]] for horizontal
    \end{itemize}
    
    Solution: Data augmentation (train with rotated images) or specialized architectures
    }
    }
\end{enumerate}

\newpage
\paragraph{Question 4. Convolution Operation and Hyperparameters}{\hfill (25 marks)}\\
The professor explained: "We have stride...padding...receptive field size...we need to be careful when we are choosing filter size padding and stride because this needs to be an integer."

\begin{enumerate}[(a)]
    \item Derive and apply the formula the professor used: "size of the next layer = (W - F + 2×padding)/stride + 1." For AlexNet's first layer with input 227×227, filter size 11, stride 4, padding 0, verify the output size of 55×55. \hfill (8 marks)
    
    \answer{
    \textbf{Deriving the Output Size Formula:}
    
    \explanation{
    Let me build this formula step by step so it makes complete sense:
    
    \textbf{Step 1: Without stride or padding}
    \begin{itemize}
        \item Input width: W
        \item Filter width: F
        \item The filter starts at position 0 and slides right
        \item Last valid position: when right edge of filter reaches right edge of input
        \item This happens at position W - F
        \item Number of positions: (W - F) + 1 (the +1 includes the starting position)
    \end{itemize}
    
    \textbf{Step 2: Adding padding}
    \begin{itemize}
        \item Padding adds P pixels on each side
        \item Effective input width becomes: W + 2P
        \item Formula becomes: (W + 2P - F) + 1
    \end{itemize}
    
    \textbf{Step 3: Adding stride}
    \begin{itemize}
        \item Stride S means we skip S-1 positions between applications
        \item If we have N valid positions, with stride S we only use every Sth position
        \item Number of actual positions: N/S
        \item Formula becomes: $\frac{(W + 2P - F)}{S} + 1$
    \end{itemize}
    }
    
    \textbf{Final Formula:}
    \[\text{Output Size} = \left\lfloor\frac{W - F + 2P}{S}\right\rfloor + 1\]
    
    The floor function $\lfloor \rfloor$ ensures we get an integer (can't have fractional neurons!)
    
    \textbf{AlexNet First Layer Calculation:}
    
    \explanation{
    Given:
    \begin{itemize}
        \item W = 227 (input width)
        \item F = 11 (filter size)
        \item S = 4 (stride)
        \item P = 0 (no padding)
    \end{itemize}
    
    Calculation:
    \begin{align}
    \text{Output Size} &= \left\lfloor\frac{227 - 11 + 2(0)}{4}\right\rfloor + 1\\
    &= \left\lfloor\frac{227 - 11}{4}\right\rfloor + 1\\
    &= \left\lfloor\frac{216}{4}\right\rfloor + 1\\
    &= \lfloor 54 \rfloor + 1\\
    &= 54 + 1\\
    &= 55
    \end{align}
    
    ✓ Verified: Output is 55×55 as expected!
    }
    
    \textbf{Why Integer Constraint Matters:}
    
    \explanation{
    If we had used input size 225:
    \begin{align}
    \frac{225 - 11 + 0}{4} + 1 = \frac{214}{4} + 1 = 53.5 + 1 = 54.5
    \end{align}
    
    Can't have 54.5 neurons! This is why the professor emphasized being careful with hyperparameters. Frameworks typically floor the result, but it's better to design for exact integers.
    }
    }
    
    \item Analyze the professor's AlexNet example where "stride is four...we are reducing dimensionality by a factor of four that is a huge reduction." Explain why this large stride works in early layers but would be problematic in later layers, citing his explanation about information redundancy. \hfill (10 marks)
    
    \answer{
    \textbf{Understanding Stride 4 in AlexNet's First Layer:}
    
    \explanation{
    The numbers tell the story:
    \begin{itemize}
        \item Input: 227×227 = 51,529 spatial positions
        \item Output: 55×55 = 3,025 spatial positions
        \item Reduction: 51,529 / 3,025 = 17× fewer positions!
        \item This is approximately $4^2 = 16×$ (stride affects both dimensions)
    \end{itemize}
    }
    
    \textbf{Why Large Stride Works in Early Layers:}
    
    \explanation{
    \textbf{1. High Spatial Redundancy in Natural Images:}
    \begin{itemize}
        \item Adjacent pixels often have similar values (smooth surfaces, gradients)
        \item A 4×4 pixel region might be almost uniform (sky, wall, skin)
        \item Sampling every 4th pixel captures essential information without much loss
        \item Think of it like downsampling audio - works if signal is smooth
    \end{itemize}
    
    \textbf{2. Computational Efficiency:}
    \begin{itemize}
        \item Reduces spatial dimensions early → fewer computations in all subsequent layers
        \item AlexNet was designed when GPUs had limited memory
        \item 17× reduction in first layer saves enormous computation throughout network
    \end{itemize}
    
    \textbf{3. Large Receptive Fields Help:}
    \begin{itemize}
        \item 11×11 filter sees substantial context
        \item Can detect meaningful patterns even when skipping pixels
        \item Like reading by scanning words, not individual letters
    \end{itemize}
    }
    
    \textbf{Why Large Stride Fails in Later Layers:}
    
    \explanation{
    \textbf{1. Information is Already Compressed:}
    \begin{itemize}
        \item Early layers: pixels → edges (high redundancy)
        \item Later layers: object parts → objects (low redundancy)
        \item Each spatial position in deep layers represents complex, unique features
        \item Skipping positions = losing critical information
    \end{itemize}
    
    \textbf{2. Spatial Resolution Already Low:}
    \begin{itemize}
        \item Layer 5 might be 13×13
        \item Stride 4 would give 3×3 output
        \item Too coarse to maintain spatial relationships
        \item Can't tell where objects are located
    \end{itemize}
    
    \textbf{3. Breaking Feature Hierarchies:}
    \begin{itemize}
        \item Deep features have precise spatial arrangements
        \item "Eye above nose above mouth" for faces
        \item Large strides destroy these relationships
        \item Network loses ability to recognize structured patterns
    \end{itemize}
    }
    
    \textbf{Modern Best Practices:}
    
    \explanation{
    \begin{itemize}
        \item Early layers: Stride 2 is common (more conservative than AlexNet)
        \item Middle layers: Stride 2 occasionally for dimension reduction
        \item Late layers: Stride 1 almost always (preserve spatial information)
        \item Alternative: Use pooling for gradual reduction instead of large strides
    \end{itemize}
    
    The professor's key insight: Information density increases with depth, so aggressive subsampling must happen early or not at all.
    }
    }
    
    \item The professor discussed the channel dimension: "when we say a two-dimensional filter actually it might have a third dimension...that spans the channels of its input layer." For 96 filters of size 11×11×3, calculate the total parameters and explain how "different filters...learned to extract different types of information." \hfill (7 marks)
    
    \answer{
    \textbf{Understanding 3D Filters in CNNs:}
    
    \explanation{
    Despite being called "2D convolution," filters are actually 3D:
    \begin{itemize}
        \item Spatial dimensions: 11×11 (height × width)
        \item Channel dimension: 3 (must match input channels - RGB)
        \item Each filter produces ONE output channel
        \item Need multiple filters for multiple output channels
    \end{itemize}
    }
    
    \textbf{Parameter Calculation:}
    
    \explanation{
    For 96 filters of size 11×11×3:
    
    \textbf{Per Filter:}
    \begin{itemize}
        \item Weights: 11 × 11 × 3 = 363 parameters
        \item Bias: 1 parameter
        \item Total per filter: 364 parameters
    \end{itemize}
    
    \textbf{All Filters:}
    \begin{itemize}
        \item 96 filters × 364 parameters = 34,944 parameters
        \item Or breaking it down:
        \item Weights: 96 × 11 × 11 × 3 = 34,848
        \item Biases: 96
        \item Total: 34,944 parameters
    \end{itemize}
    }
    
    \textbf{How Different Filters Extract Different Information:}
    
    \explanation{
    Each of the 96 filters learns to detect different patterns:
    
    \textbf{Color-Specific Detectors:}
    \begin{itemize}
        \item Filter 1: Strong response to red channel → detects reddish regions
        \item Filter 2: Green-blue difference → detects sky vs grass boundaries
        \item Filter 3: All channels equal → detects grayscale edges
    \end{itemize}
    
    \textbf{Edge Orientation Detectors:}
    \begin{itemize}
        \item Filters 10-20: Vertical edges at different scales
        \item Filters 21-30: Horizontal edges
        \item Filters 31-40: Diagonal edges (45°, 135°)
    \end{itemize}
    
    \textbf{Texture Detectors:}
    \begin{itemize}
        \item Filter 50: Checkerboard patterns
        \item Filter 51: Dots/circles
        \item Filter 52: Stripes
    \end{itemize}
    
    \textbf{Complex Pattern Detectors:}
    \begin{itemize}
        \item Filter 90: Color blobs (face-like regions)
        \item Filter 91: Corner intersections
        \item Filter 92: Center-surround patterns
    \end{itemize}
    }
    
    \textbf{Visualization Insight:}
    
    \explanation{
    When researchers visualize learned filters:
    \begin{itemize}
        \item Each filter shows what pattern maximally activates it
        \item They discover Gabor-like filters (similar to biological vision)
        \item Filters automatically specialize without explicit programming
        \item This emergent specialization is what makes deep learning powerful
    \end{itemize}
    
    The professor's point: We don't manually design these 96 filters. Through training, they automatically learn to decompose visual information into useful components, just like the visual cortex!
    }
    }
\end{enumerate}

\newpage
\paragraph{Question 5. Alternative Convolution Types}{\hfill (28 marks)}\\
The professor introduced multiple convolution variants: "We have different ways to actually restrict connectivity and share parameters in a layer."

\begin{enumerate}[(a)]
    \item Compare unshared convolution, dilated convolution, and transposed convolution. For each, explain the professor's rationale: unshared for problems without equivariance needs, dilated for increasing "effective coverage...without increasing number of parameters," and transposed for "upsampling." \hfill (12 marks)
    
    \answer{
    \textbf{1. Unshared Convolution (Locally Connected Layers):}
    
    \explanation{
    \textbf{How it works:}
    \begin{itemize}
        \item Like standard convolution: restricted connectivity (small receptive fields)
        \item Unlike standard convolution: NO parameter sharing
        \item Each spatial position has its OWN set of weights
        \item Position (0,0) uses weights $W_{0,0}$, position (0,1) uses different weights $W_{0,1}$
    \end{itemize}
    
    \textbf{Professor's Rationale - No Equivariance Needed:}
    \begin{itemize}
        \item Use when different positions should be treated differently
        \item Example: Face recognition with aligned faces
        \item Eyes always at top, mouth always at bottom
        \item Want specialized detectors for each facial region
        \item Eye detector at mouth position would be useless!
    \end{itemize}
    
    \textbf{Trade-offs:}
    \begin{itemize}
        \item Pro: Can learn position-specific patterns
        \item Pro: More expressive for structured inputs
        \item Con: Many more parameters (loses sharing benefit)
        \item Con: Requires aligned/normalized inputs
    \end{itemize}
    }
    
    \textbf{2. Dilated Convolution (Atrous Convolution):}
    
    \explanation{
    \textbf{How it works:}
    \begin{itemize}
        \item Standard 3×3 filter: samples at positions (0,0), (0,1), (0,2), (1,0), etc.
        \item Dilated 3×3 filter (dilation=2): samples at (0,0), (0,2), (0,4), (2,0), etc.
        \item "Spreads out" the filter with gaps between samples
        \item Like poking holes (à trous = "with holes" in French)
    \end{itemize}
    
    \textbf{Professor's Rationale - Increase Coverage Without More Parameters:}
    \begin{itemize}
        \item 3×3 filter with dilation 2 → covers 5×5 area
        \item 3×3 filter with dilation 4 → covers 9×9 area
        \item Still only 9 parameters regardless of dilation!
        \item Effective receptive field = (k-1) × dilation + 1
    \end{itemize}
    
    \textbf{Use cases:}
    \begin{itemize}
        \item Semantic segmentation: need wide context while maintaining resolution
        \item Replace pooling layers: increase receptive field without losing resolution
        \item Multi-scale processing: stack different dilations for multi-scale features
    \end{itemize}
    }
    
    \textbf{3. Transposed Convolution (Deconvolution):}
    
    \explanation{
    \textbf{How it works:}
    \begin{itemize}
        \item Standard convolution: large input → small output (downsampling)
        \item Transposed convolution: small input → large output (upsampling)
        \item Not the mathematical inverse! Just transposes the connectivity pattern
        \item Each input pixel influences multiple output pixels
    \end{itemize}
    
    \textbf{Professor's Rationale - Upsampling:}
    \begin{itemize}
        \item Need to increase spatial resolution (opposite of pooling)
        \item Example: 2×2 input → 4×4 output
        \item Learnable upsampling (better than bilinear interpolation)
        \item Used in generative models, segmentation decoders
    \end{itemize}
    
    \textbf{Mathematical relationship:}
    \begin{itemize}
        \item If conv: $n×n → m×m$ with kernel $k$, stride $s$
        \item Then transposed conv: $m×m → n×n$ with same $k$, $s$
        \item Output size = (input-1) × stride + kernel
    \end{itemize}
    
    \textbf{Common issue:}
    \begin{itemize}
        \item Can create "checkerboard artifacts" due to uneven overlap
        \item Solution: Use resize + conv instead of transposed conv
    \end{itemize}
    }
    }
    
    \item Analyze separable convolution as the professor explained: "we can write a 3×3 matrix as a multiplication of two vectors...we can reduce the number of parameters...from nine parameters...to six parameters." Explain depthwise separable convolution and its efficiency benefits. \hfill (10 marks)
    
    \answer{
    \textbf{Spatial Separable Convolution (Professor's Example):}
    
    \explanation{
    The professor's insight: Some filters can be decomposed into simpler components.
    
    \textbf{Example - Separable 3×3 Filter:}
    \begin{itemize}
        \item Original filter: 
        $\begin{bmatrix} 1 & 2 & 1 \\ 2 & 4 & 2 \\ 1 & 2 & 1 \end{bmatrix}$
        \item Can be written as: $\begin{bmatrix} 1 \\ 2 \\ 1 \end{bmatrix} \times \begin{bmatrix} 1 & 2 & 1 \end{bmatrix}$
        \item Vertical vector (3 params) × Horizontal vector (3 params) = 6 params total
        \item Original had 9 parameters → 33\% reduction
    \end{itemize}
    
    \textbf{How to apply:}
    \begin{itemize}
        \item First: Convolve with vertical vector [1, 2, 1] (1D convolution)
        \item Then: Convolve with horizontal vector [1, 2, 1] (1D convolution)
        \item Result identical to original 2D convolution
        \item Computation: 6 multiplications instead of 9 per position
    \end{itemize}
    
    \textbf{Limitation:} Not all filters are separable! Only works for specific patterns.
    }
    
    \textbf{Depthwise Separable Convolution (Modern Approach):}
    
    \explanation{
    Different concept: Separate spatial and channel-wise operations.
    
    \textbf{Standard Convolution Problem:}
    \begin{itemize}
        \item Input: H×W×C_in (e.g., 32×32×128)
        \item Filter: K×K×C_in×C_out (e.g., 3×3×128×256)
        \item Parameters: 3×3×128×256 = 294,912
        \item Mixes spatial and channel operations together
    \end{itemize}
    
    \textbf{Depthwise Separable Solution:}
    
    Step 1 - Depthwise Convolution:
    \begin{itemize}
        \item Apply one K×K filter per input channel
        \item 128 input channels → 128 filters of size 3×3
        \item Parameters: 3×3×128 = 1,152
        \item Output: H×W×128 (same channels)
    \end{itemize}
    
    Step 2 - Pointwise Convolution (1×1):
    \begin{itemize}
        \item Mix channels using 1×1 convolution
        \item 128 input → 256 output channels
        \item Parameters: 1×1×128×256 = 32,768
        \item Output: H×W×256
    \end{itemize}
    
    \textbf{Total: 1,152 + 32,768 = 33,920 parameters}
    \textbf{Reduction: 294,912 / 33,920 = 8.7× fewer parameters!}
    }
    
    \textbf{Efficiency Analysis:}
    
    \explanation{
    \textbf{Parameter Reduction:}
    \begin{itemize}
        \item Standard: $K^2 \times C_{in} \times C_{out}$
        \item Depthwise Separable: $K^2 \times C_{in} + C_{in} \times C_{out}$
        \item Ratio: $\frac{1}{C_{out}} + \frac{1}{K^2}$
        \item For K=3, C_out=256: ~9× reduction
    \end{itemize}
    
    \textbf{Computational Reduction:}
    \begin{itemize}
        \item Similar ratio for FLOPs
        \item Enables efficient networks: MobileNet, Xception
        \item Critical for mobile/edge deployment
    \end{itemize}
    
    \textbf{Key Insight:}
    The professor showed that we can decompose expensive operations into cheaper components without significant accuracy loss. This principle drives modern efficient architectures.
    }
    }
    
    \item The professor described 1×1 convolution as controlling "the number of channels...without using any...without combining any information from a neighborhood." Explain how this operation works and its role in efficient architectures like GoogleNet that will be discussed later. \hfill (6 marks)
    
    \answer{
    \textbf{Understanding 1×1 Convolution:}
    
    \explanation{
    At first, "1×1 convolution" sounds pointless - what can a 1×1 filter do?
    The key: It operates on the CHANNEL dimension, not spatial dimensions.
    }
    
    \textbf{How 1×1 Convolution Works:}
    
    \explanation{
    \textbf{Operation:}
    \begin{itemize}
        \item Input: H×W×C_in (e.g., 32×32×128)
        \item 1×1 Conv with C_out filters: H×W×C_out (e.g., 32×32×64)
        \item Each output channel is a weighted combination of ALL input channels
        \item At each spatial position independently
    \end{itemize}
    
    \textbf{Mathematical View:}
    For each spatial position (i,j):
    \begin{itemize}
        \item Input vector: [c1, c2, ..., c128] (channel values at that position)
        \item Output: Linear transformation (matrix multiply)
        \item Like a fully connected layer applied at each position!
    \end{itemize}
    
    \textbf{No Neighborhood Information:}
    As the professor emphasized:
    \begin{itemize}
        \item Only looks at one spatial position at a time
        \item Cannot detect edges, textures, or spatial patterns
        \item Purely channel-wise operation
    \end{itemize}
    }
    
    \textbf{Three Critical Roles in Efficient Architectures:}
    
    \explanation{
    \textbf{1. Channel Dimension Reduction (Compression):}
    \begin{itemize}
        \item Before expensive operations: 256 channels → 64 channels
        \item Apply expensive 5×5 convolution on 64 channels (not 256)
        \item Reduces computation by 4×
        \item GoogleNet's "bottleneck" design
    \end{itemize}
    
    \textbf{2. Channel Dimension Expansion:}
    \begin{itemize}
        \item After feature extraction: 64 channels → 256 channels
        \item Increases network capacity without spatial computation
        \item Cheap way to add expressiveness
    \end{itemize}
    
    \textbf{3. Cross-Channel Information Mixing:}
    \begin{itemize}
        \item Combines information across channels
        \item Learns correlations: "red + circular = apple"
        \item Essential after depthwise convolution (which doesn't mix channels)
    \end{itemize}
    }
    
    \textbf{GoogleNet (Inception) Example:}
    
    \explanation{
    Inception module uses 1×1 convolutions brilliantly:
    \begin{itemize}
        \item Input: 256 channels
        \item Branch 1: 1×1 conv → 64 channels → 3×3 conv
        \item Branch 2: 1×1 conv → 64 channels → 5×5 conv
        \item Branch 3: 1×1 conv → 128 channels (direct)
        \item Without 1×1: 256×3×3 + 256×5×5 = 8,704 params
        \item With 1×1: 256×64 + 64×3×3 + 256×64 + 64×5×5 = 35,392 params
        \item But processes 4× fewer channels in expensive convolutions!
    \end{itemize}
    
    The professor's insight: 1×1 convolutions provide a cheap way to manipulate the channel dimension, enabling more efficient and deeper networks.
    }
    }
\end{enumerate}

\newpage
\paragraph{Question 6. Deformable Convolution and Advanced Concepts}{\hfill (30 marks)}\\
The professor introduced deformable convolution: "What if we learn directly from the data...a better positioning of the filter...for each position what if I estimate an offset along X and Y that would be more meaningful."

\begin{enumerate}[(a)]
    \item Explain the concept of deformable convolution as "dynamic receptive field." Describe how "for each parameter...we need to have two offsets for X and Y delta X and delta Y" and why this makes the operation "very expensive." \hfill (12 marks)
    
    \answer{
    \textbf{The Limitation of Standard Convolution:}
    
    \explanation{
    Standard convolution uses a rigid grid:
    \begin{itemize}
        \item 3×3 filter always samples at: (-1,-1), (-1,0), (-1,1), (0,-1), (0,0), (0,1), (1,-1), (1,0), (1,1)
        \item This fixed pattern applied everywhere, regardless of image content
        \item Like using a rigid stamp - can't adapt to the actual shapes in the image
    \end{itemize}
    }
    
    \textbf{Deformable Convolution - The Core Idea:}
    
    \explanation{
    Let the network learn WHERE to look, not just WHAT to look for!
    
    \textbf{Dynamic Receptive Fields:}
    Instead of fixed positions, use learned offsets:
    \begin{itemize}
        \item Original position: (x, y)
        \item Learned offset: (Δx, Δy)
        \item Sample at: (x + Δx, y + Δy)
        \item Different offsets for each spatial location!
        \item Receptive field adapts to image content
    \end{itemize}
    }
    
    \textbf{The Offset Learning Mechanism:}
    
    \explanation{
    For a 3×3 deformable convolution:
    
    \textbf{Standard Conv:}
    \begin{itemize}
        \item 9 weight parameters (one per position)
        \item Fixed sampling positions
    \end{itemize}
    
    \textbf{Deformable Conv:}
    \begin{itemize}
        \item 9 weight parameters (unchanged)
        \item PLUS 18 offset parameters:
        \item 2 offsets (Δx, Δy) for each of 9 positions
        \item Offsets are LEARNED from data
        \item Different offsets at each spatial location
    \end{itemize}
    
    \textbf{Architecture:}
    \begin{itemize}
        \item Input → Offset prediction branch (conv layer)
        \item Outputs 18 channels (2 × 9 offsets)
        \item These offsets deform the sampling grid
        \item Main convolution uses deformed positions
    \end{itemize}
    }
    
    \textbf{Why It's "Very Expensive":}
    
    \explanation{
    \textbf{1. Additional Network Branch:}
    \begin{itemize}
        \item Need separate convolution to predict offsets
        \item Adds parameters and computation
        \item Must run before main convolution
    \end{itemize}
    
    \textbf{2. Irregular Memory Access:}
    \begin{itemize}
        \item Standard conv: predictable memory pattern (GPU-friendly)
        \item Deformable: random access based on learned offsets
        \item Breaks cache locality
        \item Can't use optimized convolution implementations
    \end{itemize}
    
    \textbf{3. Bilinear Interpolation:}
    \begin{itemize}
        \item Offsets are real numbers, not integers
        \item (x + 0.7, y + 1.3) doesn't land on pixel grid
        \item Must interpolate between 4 nearest pixels
        \item Adds computation for every sample
    \end{itemize}
    
    \textbf{4. Gradient Computation:}
    \begin{itemize}
        \item Backprop through both weights AND offsets
        \item Gradients flow through bilinear interpolation
        \item More complex computational graph
        \item ~3-4× slower than standard convolution
    \end{itemize}
    
    \textbf{Concrete Example:}
    Standard 3×3 conv on 256 channels:
    \begin{itemize}
        \item Operations: 9 × 256 = 2,304 per position
    \end{itemize}
    
    Deformable 3×3 conv:
    \begin{itemize}
        \item Offset prediction: ~2,304 operations
        \item Bilinear interpolation: 4 × 9 × 256 = 9,216 operations
        \item Main convolution: 2,304 operations
        \item Total: ~13,824 operations (6× more!)
    \end{itemize}
    }
    }
    
    \item The professor showed the benefit: "receptive fields are adjusted automatically...in such a way that receptive field actually pays attention to the most relevant content." Analyze the sheep example and explain why this improves upon vanilla convolution's "rigid...regular" placement. \hfill (8 marks)
    
    \answer{
    \textbf{The Sheep Example - Visualizing Adaptive Receptive Fields:}
    
    \explanation{
    The professor's sheep example perfectly illustrates the power of deformable convolution:
    
    \textbf{Standard Convolution on a Sheep:}
    \begin{itemize}
        \item 3×3 grid samples fixed positions
        \item Near sheep's body: some points hit the sheep, others hit background
        \item Near legs: grid might sample ground, leg, and sky simultaneously
        \item The rigid grid doesn't respect object boundaries
        \item Mixes relevant (sheep) and irrelevant (background) information
    \end{itemize}
    
    \textbf{Deformable Convolution on a Sheep:}
    \begin{itemize}
        \item Sampling points ADAPT to follow sheep's contour
        \item Near body: points cluster on the woolly texture
        \item Near legs: points align along the thin leg structure
        \item Near head: points focus on facial features
        \item Avoids sampling background pixels
    \end{itemize}
    }
    
    \textbf{Why This Improves Recognition:}
    
    \explanation{
    \textbf{1. Feature Purity:}
    \begin{itemize}
        \item Standard conv: "50\% sheep, 50\% grass" → confused features
        \item Deformable: "100\% sheep" → clean features
        \item Better signal-to-noise ratio
        \item More discriminative representations
    \end{itemize}
    
    \textbf{2. Shape Adaptation:}
    \begin{itemize}
        \item Objects have irregular shapes
        \item Rectangular grids don't match natural boundaries
        \item Deformable conv learns object-aware sampling
        \item Can follow curves, handle thin structures
    \end{itemize}
    
    \textbf{3. Scale Adaptation:}
    \begin{itemize}
        \item Small objects: offsets bring samples closer
        \item Large objects: offsets spread samples wider
        \item Automatic scale handling without multiple filter sizes
    \end{itemize}
    
    \textbf{4. Context-Aware Processing:}
    \begin{itemize}
        \item Different patterns need different sampling strategies
        \item Texture: dense, regular sampling
        \item Edges: aligned sampling along boundaries
        \item Network learns optimal strategy per location
    \end{itemize}
    }
    
    \textbf{Concrete Benefits in Practice:}
    
    \explanation{
    \textbf{Object Detection:}
    \begin{itemize}
        \item Better bounding box predictions
        \item Handles occlusion better (focuses on visible parts)
        \item Improves small object detection
    \end{itemize}
    
    \textbf{Semantic Segmentation:}
    \begin{itemize}
        \item More accurate boundaries
        \item Better handling of thin structures (poles, legs)
        \item Reduced bleeding across object boundaries
    \end{itemize}
    
    The professor's key insight: By making receptive fields adaptive, we let the network focus on what matters, dramatically improving feature quality.
    }
    }
    
    \item Solve the backpropagation challenge the professor discussed: "indices don't enter the calculations...we cannot do that because it is not part of the calculation." Explain how bilinear interpolation makes "everything differentiable" by incorporating offsets into the computation. \hfill (10 marks)
    
    \answer{
    \textbf{The Fundamental Problem - Non-Differentiable Sampling:}
    
    \explanation{
    \textbf{Integer Indexing Problem:}
    Imagine trying to sample at learned positions:
    \begin{itemize}
        \item Offset prediction: (Δx, Δy) = (1.7, 2.3)
        \item Round to integers: (2, 2)
        \item Sample: value = image[x+2, y+2]
    \end{itemize}
    
    \textbf{Why This Breaks Backpropagation:}
    \begin{itemize}
        \item The rounding operation has zero gradient everywhere
        \item $\frac{\partial \text{round}(1.7)}{\partial \text{input}} = 0$
        \item Offset (1.7, 2.3) and (1.9, 2.4) both round to (2, 2)
        \item No gradient signal to improve offset predictions!
        \item The professor's point: "indices don't enter calculations"
    \end{itemize}
    }
    
    \textbf{The Solution - Bilinear Interpolation:}
    
    \explanation{
    Instead of rounding, use the fractional parts to blend nearby pixels!
    
    \textbf{How Bilinear Interpolation Works:}
    For offset (Δx, Δy) = (1.7, 2.3):
    \begin{itemize}
        \item Don't round! Keep fractional parts
        \item Find 4 nearest pixels: (1,2), (2,2), (1,3), (2,3)
        \item Calculate weights based on distance
        \item Blend all 4 values
    \end{itemize}
    
    \textbf{The Mathematics:}
    Let p = (x + Δx, y + Δy) be the target position
    \begin{itemize}
        \item Top-left pixel: $(⌊p_x⌋, ⌊p_y⌋)$, weight = $(1-\{p_x\})(1-\{p_y\})$
        \item Top-right pixel: $(⌊p_x⌋+1, ⌊p_y⌋)$, weight = $\{p_x\}(1-\{p_y\})$
        \item Bottom-left pixel: $(⌊p_x⌋, ⌊p_y⌋+1)$, weight = $(1-\{p_x\})\{p_y\}$
        \item Bottom-right pixel: $(⌊p_x⌋+1, ⌊p_y⌋+1)$, weight = $\{p_x\}\{p_y\}$
    \end{itemize}
    Where $\{a\}$ means fractional part of $a$
    
    Output = Σ(pixel_value × weight)
    }
    
    \textbf{Why This Is Differentiable:}
    
    \explanation{
    \textbf{Key Insight: Offsets appear in the weight calculations!}
    
    Example with (1.7, 2.3):
    \begin{itemize}
        \item Fractional parts: fx = 0.7, fy = 0.3
        \item Weight for pixel (2,2) = fx × (1-fy) = 0.7 × 0.7 = 0.49
        \item If Δx increases by ε: fx becomes 0.7+ε
        \item New weight = (0.7+ε) × 0.7 = 0.49 + 0.7ε
        \item Gradient: $\frac{\partial \text{weight}}{\partial \Delta x} = 0.7$
    \end{itemize}
    
    \textbf{Complete Gradient Flow:}
    \begin{itemize}
        \item Loss → Output activations
        \item → Sampled values (via conv weights)
        \item → Interpolation weights (via bilinear formula)
        \item → Offsets (Δx, Δy)
        \item → Offset prediction network
    \end{itemize}
    }
    
    \textbf{The Beautiful Result:}
    
    \explanation{
    \textbf{Smooth Optimization Landscape:}
    \begin{itemize}
        \item Small offset changes → small output changes
        \item Gradient descent can fine-tune positions
        \item Network learns to move sampling points to useful locations
    \end{itemize}
    
    \textbf{Automatic Differentiation:}
    \begin{itemize}
        \item Modern frameworks (PyTorch, TensorFlow) handle this automatically
        \item Just implement forward pass with bilinear interpolation
        \item Backprop computes all gradients correctly
    \end{itemize}
    
    The professor's key insight: By making sampling positions "part of the calculation" through interpolation weights, we enable end-to-end learning of WHERE to look, not just WHAT to look for. This transforms a discrete, non-differentiable operation into a smooth, optimizable one.
    }
    }
\end{enumerate}

\vfill
\begin{center}{\bf END OF ANSWERED EXAM}\end{center>
\end{document}