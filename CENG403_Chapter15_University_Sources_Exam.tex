\documentclass[12pt]{article}
\usepackage{fancyhdr}
\usepackage{amsmath,amsfonts,enumerate}
\usepackage{color,graphicx}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}
\usetikzlibrary{arrows,positioning,shapes,calc,matrix}
\pagestyle{fancy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Course customization based on university sources
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\masunitnumber}{CENG 403}
\newcommand{\examdate}{January 2025}
\newcommand{\academicyear}{2024-2025}
\newcommand{\semester}{I}
\newcommand{\coursename}{Deep Learning - Large Language Models \& Vision Transformers (University Sources)}
\newcommand{\numberofhours}{3}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CUSTOM SPACING COMMANDS FOR ANSWER SPACES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\answerspace}[1]{\vspace{#1}}
\newcommand{\questionspace}{\vspace{3cm}}        
\newcommand{\subquestionspace}{\vspace{2.5cm}}   
\newcommand{\shortanswer}{\vspace{2cm}}          
\newcommand{\mediumanswer}{\vspace{3cm}}         
\newcommand{\longanswer}{\vspace{4cm}}           
\newcommand{\journalspace}{\vspace{4.5cm}}       
\newcommand{\codespace}{\vspace{5cm}}            
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Header setup
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lhead{}
\rhead{}
\chead{{\bf MIDDLE EAST TECHNICAL UNIVERSITY}}
\lfoot{}
\rfoot{}
\cfoot{}
\begin{document}
\setlength{\headsep}{5truemm}
\setlength{\headheight}{14.5truemm}
\setlength{\voffset}{-0.45truein}
\renewcommand{\headrulewidth}{0.0pt}
\begin{center}
SEMESTER \semester\ EXAMINATION \academicyear
\end{center}
\begin{center}
{\bf \masunitnumber\ -- \coursename}
\end{center}
\vspace{20truemm}
\noindent \examdate\hspace{45truemm} TIME ALLOWED: \numberofhours\ HOURS
\vspace{19truemm}
\hrule
\vspace{19truemm}
\noindent\underline{INSTRUCTIONS TO CANDIDATES}
\vspace{8truemm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Instructions based on university standards
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate}
\item This examination paper contains {\bf SEVEN (7)} questions and comprises 
{\bf TEN (10)} printed pages.
\item Answer all questions. 
The marks for each question are indicated at the beginning of each question.
\item Answer each question beginning on a {\bf FRESH} page of the answer book.
\item This {\bf IS NOT an OPEN BOOK} exam.
\item Show all mathematical derivations clearly with proper notation.
\item For implementation questions, provide clear pseudocode or algorithms.
\item Explain computational complexity where requested.
\item Draw clear architectural diagrams with proper labels.
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% New page for questions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\lhead{}
\rhead{\masunitnumber}
\chead{}
\lfoot{}
\cfoot{\thepage}
\rfoot{}
\setlength{\footskip}{45pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EXAM QUESTIONS BASED ON UNIVERSITY SOURCES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Question 1. Large Language Model Pre-training Strategies}\hfill (28 marks)\\
Based on foundational LLM research and university courses on language modeling.

\begin{enumerate}[(a)]
    \item Compare and contrast autoregressive language modeling (GPT-style) versus masked language modeling (BERT-style) pre-training objectives: \hfill (12 marks)
    \begin{itemize}
        \item Mathematical formulation of each objective
        \item Advantages and disadvantages of each approach
        \item Computational complexity comparison
        \item Suitability for different downstream tasks
    \end{itemize}
    
    \journalspace
    
    \item Design a hybrid pre-training strategy that combines both autoregressive and masked language modeling. Explain: \hfill (10 marks)
    \begin{itemize}
        \item How to modify the transformer architecture to support both objectives
        \item Training procedure and loss function combination
        \item Expected benefits over single-objective pre-training
    \end{itemize}
    
    \mediumanswer
    
    \item Analyze the scaling laws for language models. Given a computational budget $C$, derive the optimal allocation between model parameters $N$, dataset size $D$, and training compute, considering the relationship: \hfill (6 marks)
    $$L(N, D) = A + \frac{B}{N^{\alpha}} + \frac{C}{D^{\beta}}$$
    
    \mediumanswer
\end{enumerate}

\newpage
\paragraph{Question 2. Advanced Training Techniques for Large Language Models}\hfill (30 marks)\\
Based on recent advances in LLM training methodologies.

\begin{enumerate}[(a)]
    \item Implement a complete Reinforcement Learning from Human Feedback (RLHF) training pipeline: \hfill (15 marks)
    \begin{itemize}
        \item Supervised fine-tuning phase algorithm
        \item Reward model training with pairwise preference data
        \item PPO (Proximal Policy Optimization) for policy training
        \item KL divergence regularization to prevent mode collapse
    \end{itemize}
    
    \codespace
    
    \item Analyze the constitutional AI approach for alignment. Design a self-critique and revision process: \hfill (8 marks)
    \begin{itemize}
        \item Constitutional principles formulation
        \item Self-evaluation criteria
        \item Iterative improvement mechanism
    \end{itemize}
    
    \mediumanswer
    
    \item Compare different alignment techniques: RLHF, Constitutional AI, and Direct Preference Optimization (DPO). Discuss: \hfill (7 marks)
    \begin{itemize}
        \item Theoretical foundations of each approach
        \item Practical implementation challenges
        \item Scalability and effectiveness trade-offs
    \end{itemize}
    
    \mediumanswer
\end{enumerate}

\newpage
\paragraph{Question 3. Vision Transformer Architecture and Optimization}\hfill (25 marks)\\
Based on Vision Transformer research and computer vision applications.

\begin{enumerate}[(a)]
    \item Design a hierarchical Vision Transformer architecture similar to Swin Transformer: \hfill (12 marks)
    \begin{itemize}
        \item Patch partitioning and merging strategy
        \item Window-based attention mechanism
        \item Shifted window attention for cross-window connections
        \item Multi-scale feature extraction
    \end{itemize}
    
    \journalspace
    
    \item Implement masked autoencoder pre-training for Vision Transformers: \hfill (8 marks)
    \begin{itemize}
        \item Random patch masking strategy (75-85\% masking ratio)
        \item Encoder-decoder architecture design
        \item Reconstruction loss formulation
        \item Fine-tuning procedure for downstream tasks
    \end{itemize}
    
    \mediumanswer
    
    \item Analyze the computational trade-offs between Vision Transformers and Convolutional Neural Networks: \hfill (5 marks)
    \begin{itemize}
        \item FLOPs comparison for different input resolutions
        \item Memory requirements analysis
        \item Inductive bias comparison
    \end{itemize}
    
    \shortanswer
\end{enumerate}

\newpage
\paragraph{Question 4. Multimodal Language Models and Cross-Modal Learning}\hfill (22 marks)\\
Based on recent advances in vision-language and multimodal models.

\begin{enumerate}[(a)]
    \item Design a multimodal transformer architecture for vision-language understanding: \hfill (12 marks)
    \begin{itemize}
        \item Image and text tokenization strategies
        \item Cross-modal attention mechanisms
        \item Fusion approaches (early, late, intermediate)
        \item Pre-training objectives for multimodal learning
    \end{itemize}
    
    \journalspace
    
    \item Implement a contrastive learning framework for vision-language representation learning (similar to CLIP): \hfill (6 marks)
    \begin{itemize}
        \item Dual encoder architecture
        \item Contrastive loss formulation
        \item Negative sampling strategies
        \item Zero-shot transfer capabilities
    \end{itemize}
    
    \mediumanswer
    
    \item Analyze the challenges of multimodal reasoning and propose solutions for: \hfill (4 marks)
    \begin{itemize}
        \item Cross-modal alignment
        \item Compositional understanding
        \item Temporal reasoning in video-language tasks
    \end{itemize}
    
    \shortanswer
\end{enumerate}

\newpage
\paragraph{Question 5. Efficient Transformer Architectures and Optimization}\hfill (20 marks)\\
Based on research on efficient attention mechanisms and model compression.

\begin{enumerate}[(a)]
    \item Compare different efficient attention mechanisms: \hfill (10 marks)
    \begin{itemize}
        \item Sparse attention patterns (local, strided, global)
        \item Linear attention approximations
        \item Low-rank attention factorization
        \item Performer and other kernel-based methods
    \end{itemize}
    
    Provide complexity analysis and accuracy trade-offs for each.
    
    \mediumanswer
    
    \item Design a mixture of experts (MoE) transformer architecture: \hfill (6 marks)
    \begin{itemize}
        \item Expert routing mechanism
        \item Load balancing strategies
        \item Training stability techniques
    \end{itemize}
    
    \mediumanswer
    
    \item Implement knowledge distillation for transformer compression: \hfill (4 marks)
    \begin{itemize}
        \item Teacher-student architecture design
        \item Distillation loss formulation
        \item Layer-wise knowledge transfer
    \end{itemize}
    
    \shortanswer
\end{enumerate}

\newpage
\paragraph{Question 6. Emergent Capabilities and Scaling Phenomena}\hfill (18 marks)\\
Based on research on emergent behaviors in large-scale models.

\begin{enumerate}[(a)]
    \item Analyze emergent capabilities in large language models: \hfill (8 marks)
    \begin{itemize}
        \item Few-shot learning abilities
        \item Chain-of-thought reasoning
        \item In-context learning mechanisms
        \item Theory of mind capabilities
    \end{itemize}
    
    Discuss the relationship between model scale and capability emergence.
    
    \mediumanswer
    
    \item Design evaluation frameworks for assessing: \hfill (6 marks)
    \begin{itemize}
        \item Reasoning capabilities (mathematical, logical, commonsense)
        \item Factual knowledge retention and updating
        \item Bias and fairness in model outputs
    \end{itemize}
    
    \mediumanswer
    
    \item Analyze the phenomenon of "grokking" in transformer training: \hfill (4 marks)
    \begin{itemize}
        \item Delayed generalization patterns
        \item Phase transitions in learning
        \item Implications for training dynamics
    \end{itemize}
    
    \shortanswer
\end{enumerate}

\newpage
\paragraph{Question 7. Future Directions and Advanced Applications}\hfill (22 marks)\\
Based on cutting-edge research and emerging applications.

\begin{enumerate}[(a)]
    \item Design a foundation model architecture for scientific computing: \hfill (10 marks)
    \begin{itemize}
        \item Multi-scale data representation (molecular to systems level)
        \item Physics-informed attention mechanisms
        \item Integration with numerical solvers
        \item Transfer learning across scientific domains
    \end{itemize}
    
    \mediumanswer
    
    \item Implement a neurosymbolic transformer that combines neural and symbolic reasoning: \hfill (8 marks)
    \begin{itemize}
        \item Neural-symbolic interface design
        \item Differentiable program synthesis
        \item Knowledge graph integration
        \item Interpretability mechanisms
    \end{itemize}
    
    \mediumanswer
    
    \item Analyze the potential and challenges of recursive self-improvement in AI systems: \hfill (4 marks)
    \begin{itemize}
        \item Automated model architecture search
        \item Self-supervised capability expansion
        \item Safety and control considerations
    \end{itemize}
    
    \shortanswer
\end{enumerate}

\vfill
\begin{center}{\bf END OF PAPER}\end{center>
\end{document>