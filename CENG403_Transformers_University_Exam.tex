\documentclass[12pt]{article}
\usepackage{fancyhdr}
\usepackage{amsmath,amsfonts,enumerate}
\usepackage{color,graphicx}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}
\usetikzlibrary{arrows,positioning,shapes,calc,matrix}
\pagestyle{fancy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Course customization
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\masunitnumber}{CENG 403}
\newcommand{\examdate}{January 2025}
\newcommand{\academicyear}{2024-2025}
\newcommand{\semester}{I}
\newcommand{\coursename}{Deep Learning - Transformers and Attention Mechanisms}
\newcommand{\numberofhours}{3}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CUSTOM SPACING COMMANDS FOR ANSWER SPACES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Flexible answer space command - you can adjust the size
\newcommand{\answerspace}[1]{\vspace{#1}}
% Standard spacing commands with predefined sizes
\newcommand{\questionspace}{\vspace{3cm}}        % Space between questions
\newcommand{\subquestionspace}{\vspace{2.5cm}}   % Standard space for sub-questions
\newcommand{\shortanswer}{\vspace{2cm}}          % For simple calculations
\newcommand{\mediumanswer}{\vspace{3cm}}         % For moderate complexity
\newcommand{\longanswer}{\vspace{4cm}}           % For complex problems
\newcommand{\journalspace}{\vspace{4.5cm}}       % For detailed explanations
\newcommand{\codespace}{\vspace{5cm}}            % For code implementations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Don't touch anything from here till instructions
% to candidates
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lhead{}
\rhead{}
\chead{{\bf MIDDLE EAST TECHNICAL UNIVERSITY}}
\lfoot{}
\rfoot{}
\cfoot{}
\begin{document}
\setlength{\headsep}{5truemm}
\setlength{\headheight}{14.5truemm}
\setlength{\voffset}{-0.45truein}
\renewcommand{\headrulewidth}{0.0pt}
\begin{center}
SEMESTER \semester\ EXAMINATION \academicyear
\end{center}
\begin{center}
{\bf \masunitnumber\ -- \coursename}
\end{center}
\vspace{20truemm}
\noindent \examdate\hspace{45truemm} TIME ALLOWED: \numberofhours\ HOURS
\vspace{19truemm}
\hrule
\vspace{19truemm}
\noindent\underline{INSTRUCTIONS TO CANDIDATES}
\vspace{8truemm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Exam instructions based on university standards
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate}
\item This examination paper contains {\bf SEVEN (7)} questions and comprises 
{\bf TEN (10)} printed pages.
\item Answer all questions. 
The marks for each question are indicated at the beginning of each question.
\item Answer each question beginning on a {\bf FRESH} page of the answer book.
\item This {\bf IS NOT an OPEN BOOK} exam.
\item Calculators are allowed for numerical computations.
\item Show all mathematical derivations and computational steps clearly.
\item For matrix operations, clearly indicate dimensions and show intermediate steps.
\item Write pseudocode clearly with proper indentation and comments.
\item Draw clear diagrams where requested and label all components.
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% leave this as it is
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\lhead{}
\rhead{\masunitnumber}
\chead{}
\lfoot{}
\cfoot{\thepage}
\rfoot{}
\setlength{\footskip}{45pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EXAM QUESTIONS BASED ON UNIVERSITY SOURCES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Question 1. Attention Mechanism Fundamentals}\hfill (20 marks)\\
Based on Stanford CS224n and MIT 6.390 course materials, answer the following about attention mechanisms.

\begin{enumerate}[(a)]
    \item Explain the four key components of an attention mechanism and describe how they interact to process sequential data. Include a discussion of how attention differs from traditional encoder-decoder approaches. \hfill (8 marks)
    
    \mediumanswer
    
    \item Derive the mathematical formulation for scaled dot-product attention. Given query $Q \in \mathbb{R}^{n \times d_k}$, key $K \in \mathbb{R}^{m \times d_k}$, and value $V \in \mathbb{R}^{m \times d_v}$ matrices, show that: \hfill (12 marks)
    $$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
    
    Explain why scaling by $\frac{1}{\sqrt{d_k}}$ is crucial for gradient stability and provide the mathematical reasoning behind this choice.
    
    \journalspace
\end{enumerate}

\newpage
\paragraph{Question 2. Computational Complexity Analysis}\hfill (18 marks)\\
Compare the computational complexities of different sequence modeling approaches.

\begin{enumerate}[(a)]
    \item Complete the complexity analysis table below for processing sequences of length $n$ with dimensionality $d$: \hfill (10 marks)
    
    \begin{center}
    \begin{tabular}{|l|c|c|c|}
    \hline
    \textbf{Operation} & \textbf{Self-Attention} & \textbf{Recurrent} & \textbf{Convolutional} \\
    \hline
    Sequential Operations & ? & $O(n)$ & ? \\
    \hline
    Maximum Path Length & ? & $O(n)$ & ? \\
    \hline
    Computational Complexity & $O(n^2 \cdot d)$ & ? & ? \\
    \hline
    \end{tabular}
    \end{center}
    
    \shortanswer
    
    \item For self-attention with sequence length $n = 1000$ and embedding dimension $d = 512$, calculate: \hfill (8 marks)
    \begin{itemize}
        \item Total number of attention parameters needed
        \item Memory complexity for storing attention weights
        \item Number of floating-point operations for one forward pass
    \end{itemize}
    
    \mediumanswer
\end{enumerate}

\newpage
\paragraph{Question 3. Multi-Head Attention Implementation}\hfill (25 marks)\\
Based on UvA Deep Learning tutorial materials, implement and analyze multi-head attention.

\begin{enumerate}[(a)]
    \item Write pseudocode for a multi-head attention layer that handles variable sequence lengths and optional masking. Your implementation should include: \hfill (15 marks)
    \begin{itemize}
        \item Input projection to multiple heads
        \item Parallel attention computation
        \item Output concatenation and projection
        \item Support for causal masking
    \end{itemize}
    
    \begin{lstlisting}[language=Python, basicstyle=\small, frame=single]
# Your pseudocode here
function MultiHeadAttention(X, num_heads, mask=None):
    // Complete this implementation
    
    
    
    
    
    
    
    return output
    \end{lstlisting}
    
    \codespace
    
    \item Prove that self-attention mechanisms are permutation-equivariant. That is, show that if $P$ is a permutation matrix, then: \hfill (10 marks)
    $$\text{SelfAttention}(PX) = P \cdot \text{SelfAttention}(X)$$
    
    \longanswer
\end{enumerate}

\newpage
\paragraph{Question 4. Positional Encoding Design}\hfill (22 marks)\\
Positional encoding allows transformers to understand sequence order without inherent positional bias.

\begin{enumerate}[(a)]
    \item Design a positional encoding mechanism for sequences up to length 1000 using trigonometric functions. Write the mathematical formula and justify your design choices. \hfill (10 marks)
    
    \mediumanswer
    
    \item Compare learnable positional embeddings versus trigonometric positional encoding: \hfill (8 marks)
    \begin{itemize}
        \item Generalization to sequences longer than training length
        \item Parameter efficiency
        \item Interpolation and extrapolation capabilities
    \end{itemize}
    
    \mediumanswer
    
    \item Given the trigonometric positional encoding formula:
    $$PE_{(pos,2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
    $$PE_{(pos,2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
    
    Calculate the positional encoding for position $pos = 3$ and dimensions $i = 0, 1$ when $d_{model} = 512$. \hfill (4 marks)
    
    \shortanswer
\end{enumerate}

\newpage
\paragraph{Question 5. Transformer Architecture and Information Flow}\hfill (28 marks)\\
Analyze the complete transformer architecture and its information processing capabilities.

\begin{enumerate}[(a)]
    \item Draw a complete transformer encoder-decoder architecture processing the translation task "Hello world" → "Hola mundo". Show: \hfill (12 marks)
    \begin{itemize}
        \item Input and output embeddings with positional encoding
        \item Multi-head self-attention in encoder
        \item Masked multi-head self-attention in decoder
        \item Cross-attention between encoder and decoder
        \item Feed-forward networks and residual connections
        \item Layer normalization placements
    \end{itemize}
    
    \begin{center}
    \begin{tikzpicture}[scale=0.6]
        % Space for students to draw complete architecture
        \draw[dotted] (0,0) rectangle (15,12);
        \node at (7.5,11.5) {\textbf{Draw complete transformer encoder-decoder here}};
        
        % Input/Output labels
        \node at (2,0.5) {\texttt{["Hello", "world"]}};
        \node at (13,0.5) {\texttt{["Hola", "mundo"]}};
        
        \node[left] at (0,6) {\textbf{Encoder}};
        \node[right] at (15,6) {\textbf{Decoder}};
    \end{tikzpicture}
    \end{center}
    
    \shortanswer
    
    \item Explain why causal masking is essential in the decoder during training. What would happen if we didn't use masking? Provide a mathematical justification. \hfill (8 marks)
    
    \mediumanswer
    
    \item Derive the gradient flow through a residual connection in a transformer layer. Show how residual connections help mitigate vanishing gradients in deep transformer networks. \hfill (8 marks)
    
    \mediumanswer
\end{enumerate}

\newpage
\paragraph{Question 6. Advanced Attention Mechanisms}\hfill (20 marks)\\
Explore advanced concepts in attention mechanisms and their applications.

\begin{enumerate}[(a)]
    \item Sparse attention patterns can reduce computational complexity. Design a sparse attention pattern for sequences of length 16 where each token attends to: \hfill (10 marks)
    \begin{itemize}
        \item Itself
        \item Previous 2 tokens
        \item Next 2 tokens  
        \item Every 4th token globally
    \end{itemize}
    
    Draw the 16×16 attention mask matrix and calculate the computational savings compared to full attention.
    
    \begin{center}
    \begin{tikzpicture}[scale=0.4]
        % 16x16 grid for students to fill
        \draw[step=0.5] (0,0) grid (8,8);
        
        % Labels
        \foreach \i in {0,...,15} {
            \node[scale=0.6] at (\i*0.5+0.25, -0.3) {\i};
            \node[scale=0.6] at (-0.3, (15-\i)*0.5+0.25) {\i};
        }
        
        \node[above] at (4,8.3) {\textbf{Design sparse attention pattern}};
        \node[below] at (4,-0.8) {\textbf{Tokens 0-15}};
        \node[left, rotate=90] at (-0.8,4) {\textbf{Tokens 0-15}};
    \end{tikzpicture}
    \end{center}
    
    \shortanswer
    
    \item Analyze the trade-offs between different attention mechanisms: \hfill (10 marks)
    \begin{itemize}
        \item Full self-attention vs. local attention windows
        \item Single-head vs. multi-head attention
        \item Additive vs. multiplicative attention
    \end{itemize}
    Discuss computational complexity, expressiveness, and practical considerations.
    
    \mediumanswer
\end{enumerate}

\newpage
\paragraph{Question 7. Practical Implementation and Optimization}\hfill (17 marks)\\
Address practical considerations in transformer implementation and optimization.

\begin{enumerate}[(a)]
    \item Design a learning rate schedule for transformer training. Implement the warm-up scheduler with cosine decay used in many transformer models: \hfill (8 marks)
    
    $$lr(step) = \begin{cases} 
    lr_{max} \cdot \frac{step}{warmup\_steps} & \text{if } step \leq warmup\_steps \\
    lr_{max} \cdot 0.5 \cdot \left(1 + \cos\left(\frac{step - warmup\_steps}{total\_steps - warmup\_steps} \pi\right)\right) & \text{otherwise}
    \end{cases}$$
    
    Plot this schedule for $lr_{max} = 0.001$, $warmup\_steps = 4000$, $total\_steps = 100000$.
    
    \mediumanswer
    
    \item Explain three key optimization techniques for transformer training: \hfill (9 marks)
    \begin{itemize}
        \item Gradient clipping and why it's necessary
        \item Layer normalization placement (pre-norm vs. post-norm)
        \item Weight initialization strategies for attention layers
    \end{itemize}
    
    \mediumanswer
\end{enumerate}

\vfill
\begin{center}{\bf END OF PAPER}\end{center}
\end{document}