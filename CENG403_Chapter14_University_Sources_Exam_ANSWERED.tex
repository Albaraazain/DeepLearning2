\documentclass[12pt]{article}
\usepackage{fancyhdr}
\usepackage{amsmath,amsfonts,enumerate}
\usepackage{color,graphicx}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}
\usetikzlibrary{arrows,positioning,shapes,calc,matrix}

% Define colors for answers
\definecolor{answercolor}{RGB}{0,100,0}
\definecolor{explanationcolor}{RGB}{0,0,139}

% Custom commands for answers
\newcommand{\answer}[1]{{\color{answercolor}\textbf{Answer:} #1}}
\newcommand{\explanation}[1]{{\color{explanationcolor}#1}}

\pagestyle{fancy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Course customization based on university sources
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\masunitnumber}{CENG 403}
\newcommand{\examdate}{January 2025}
\newcommand{\academicyear}{2024-2025}
\newcommand{\semester}{I}
\newcommand{\coursename}{Deep Learning - Self-Attention \& Transformers (University Sources) - ANSWERED}
\newcommand{\numberofhours}{3}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CUSTOM SPACING COMMANDS FOR ANSWER SPACES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\answerspace}[1]{\vspace{#1}}
\newcommand{\questionspace}{\vspace{3cm}}        
\newcommand{\subquestionspace}{\vspace{2.5cm}}   
\newcommand{\shortanswer}{\vspace{2cm}}          
\newcommand{\mediumanswer}{\vspace{3cm}}         
\newcommand{\longanswer}{\vspace{4cm}}           
\newcommand{\journalspace}{\vspace{4.5cm}}       
\newcommand{\codespace}{\vspace{5cm}}            
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Header setup
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lhead{}
\rhead{}
\chead{{\bf MIDDLE EAST TECHNICAL UNIVERSITY}}
\lfoot{}
\rfoot{}
\cfoot{}
\begin{document}
\setlength{\headsep}{5truemm}
\setlength{\headheight}{14.5truemm}
\setlength{\voffset}{-0.45truein}
\renewcommand{\headrulewidth}{0.0pt}
\begin{center}
SEMESTER \semester\ EXAMINATION \academicyear
\end{center}
\begin{center}
{\bf \masunitnumber\ -- \coursename}
\end{center>
\vspace{20truemm}
\noindent \examdate\hspace{45truemm} TIME ALLOWED: \numberofhours\ HOURS
\vspace{19truemm}
\hrule
\vspace{19truemm}
\noindent\underline{INSTRUCTIONS TO CANDIDATES}
\vspace{8truemm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Instructions based on university standards
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate}
\item This examination paper contains {\bf SEVEN (7)} questions and comprises 
{\bf TEN (10)} printed pages.
\item Answer all questions. 
The marks for each question are indicated at the beginning of each question.
\item Answer each question beginning on a {\bf FRESH} page of the answer book.
\item This {\bf IS NOT an OPEN BOOK} exam.
\item Show all mathematical derivations clearly with proper notation.
\item For implementation questions, provide clear pseudocode or algorithms.
\item Explain computational complexity where requested.
\item Draw clear architectural diagrams with proper labels.
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% New page for questions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\lhead{}
\rhead{\masunitnumber}
\chead{}
\lfoot{}
\cfoot{\thepage}
\rfoot{}
\setlength{\footskip}{45pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EXAM QUESTIONS BASED ON UNIVERSITY SOURCES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Question 1. Mathematical Foundations of Self-Attention}\hfill (25 marks)\\
Based on transformer research papers and university deep learning courses.

\begin{enumerate}[(a)]
    \item Define the self-attention mechanism mathematically. For a sequence of input vectors $X = [x_1, x_2, ..., x_n]$ where $x_i \in \mathbb{R}^d$, derive the complete attention formula including: \hfill (10 marks)
    \begin{itemize}
        \item Query, Key, Value transformations
        \item Attention weight computation
        \item Output aggregation
        \item Scaling factor justification
    \end{itemize}
    
    \answer{Complete mathematical formulation of self-attention with QKV transformations, scaled dot-product computation, and output aggregation.}
    
    \explanation{
    \textbf{Mathematical Definition of Self-Attention:}
    
    \textbf{1. Input Processing:}
    Given input sequence $X = [x_1, x_2, ..., x_n]$ where $x_i \in \mathbb{R}^d$
    
    \textbf{2. Query, Key, Value Transformations:}
    \begin{align}
        Q &= XW_Q \quad \text{where } W_Q \in \mathbb{R}^{d \times d_k} \\
        K &= XW_K \quad \text{where } W_K \in \mathbb{R}^{d \times d_k} \\
        V &= XW_V \quad \text{where } W_V \in \mathbb{R}^{d \times d_v}
    \end{align}
    
    \textbf{3. Attention Weight Computation:}
    \begin{align}
        e_{ij} &= \frac{q_i^T k_j}{\sqrt{d_k}} \quad \text{(scaled similarity)} \\
        \alpha_{ij} &= \frac{\exp(e_{ij})}{\sum_{k=1}^n \exp(e_{ik})} \quad \text{(softmax normalization)}
    \end{align}
    
    \textbf{4. Output Aggregation:}
    $$z_i = \sum_{j=1}^n \alpha_{ij} v_j$$
    
    \textbf{5. Complete Matrix Form:}
    $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
    
    \textbf{Scaling Factor Justification:}
    
    \textbf{Statistical Analysis:}
    \begin{itemize}
        \item For random vectors $q, k \sim \mathcal{N}(0, I)$ in $\mathbb{R}^{d_k}$
        \item Dot product: $q^T k = \sum_{i=1}^{d_k} q_i k_i$
        \item Expected value: $\mathbb{E}[q^T k] = 0$
        \item Variance: $\text{Var}[q^T k] = d_k$
        \item Standard deviation: $\sigma[q^T k] = \sqrt{d_k}$
    \end{itemize}
    
    \textbf{Why Scaling is Critical:}
    \begin{itemize}
        \item Large $d_k$ → large dot products → saturated softmax
        \item Saturated softmax → tiny gradients → poor learning
        \item Scaling by $\frac{1}{\sqrt{d_k}}$ normalizes variance to 1
        \item Maintains stable gradients across different model sizes
    \end{itemize}
    }
    
    \item Prove that the attention weights $\alpha_{ij}$ sum to 1 for each query position $i$. Show that $\sum_{j=1}^n \alpha_{ij} = 1$. \hfill (5 marks)
    
    \answer{Softmax normalization guarantees attention weights sum to 1 by construction.}
    
    \explanation{
    \textbf{Proof that $\sum_{j=1}^n \alpha_{ij} = 1$:}
    
    \textbf{Given:} Attention weights are computed using softmax:
    $$\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^n \exp(e_{ik})}$$
    
    \textbf{To Prove:} $\sum_{j=1}^n \alpha_{ij} = 1$ for any fixed $i$
    
    \textbf{Proof:}
    \begin{align}
        \sum_{j=1}^n \alpha_{ij} &= \sum_{j=1}^n \frac{\exp(e_{ij})}{\sum_{k=1}^n \exp(e_{ik})} \\
        &= \frac{1}{\sum_{k=1}^n \exp(e_{ik})} \sum_{j=1}^n \exp(e_{ij}) \\
        &= \frac{\sum_{j=1}^n \exp(e_{ij})}{\sum_{k=1}^n \exp(e_{ik})} \\
        &= \frac{\sum_{j=1}^n \exp(e_{ij})}{\sum_{j=1}^n \exp(e_{ij})} \quad \text{(relabeling } k \text{ as } j\text{)} \\
        &= 1
    \end{align}
    
    \textbf{Key Insight:} Softmax is designed to produce a probability distribution, ensuring all weights sum to 1 while preserving relative magnitudes of similarity scores.
    
    \textbf{Implications:}
    \begin{itemize}
        \item Each output $z_i$ is a convex combination of value vectors
        \item Attention weights represent a probability distribution over positions
        \item This property enables interpretability of attention patterns
    \end{itemize}
    }
    
    \item Analyze the computational and space complexity of self-attention for sequence length $n$ and embedding dimension $d$. Compare with RNN complexity. \hfill (6 marks)
    
    \answer{Self-attention has $O(n^2d)$ time complexity vs RNN's $O(nd^2)$, with different trade-offs for different sequence lengths.}
    
    \explanation{
    \textbf{Self-Attention Complexity Analysis:}
    
    \textbf{Time Complexity:}
    \begin{itemize}
        \item QKV projections: $O(nd^2)$ (3 matrix multiplications)
        \item Attention scores: $QK^T$ requires $O(n^2d)$ operations
        \item Softmax: $O(n^2)$ operations
        \item Output computation: $O(n^2d)$ operations
        \item \textbf{Total: $O(n^2d + nd^2)$}
    \end{itemize}
    
    \textbf{Space Complexity:}
    \begin{itemize}
        \item QKV matrices: $O(nd)$ each
        \item Attention matrix: $O(n^2)$
        \item \textbf{Total: $O(n^2 + nd)$}
    \end{itemize}
    
    \textbf{RNN Complexity Analysis:}
    
    \textbf{Time Complexity:}
    \begin{itemize}
        \item Per time step: $O(d^2)$ (hidden-to-hidden transformation)
        \item Total for sequence: $O(nd^2)$
        \item Sequential dependency prevents parallelization
    \end{itemize}
    
    \textbf{Space Complexity:}
    \begin{itemize}
        \item Hidden states: $O(nd)$ (if storing all for backprop)
        \item Or $O(d)$ if not storing intermediate states
    \end{itemize}
    
    \textbf{Comparison and Trade-offs:}
    
    \textbf{When $n << d$ (short sequences, large embeddings):}
    \begin{itemize}
        \item Self-attention: $O(nd^2)$ dominates
        \item RNN: $O(nd^2)$
        \item Similar complexity, but self-attention allows parallelization
    \end{itemize}
    
    \textbf{When $n >> d$ (long sequences, small embeddings):}
    \begin{itemize}
        \item Self-attention: $O(n^2d)$ becomes prohibitive
        \item RNN: $O(nd^2)$ remains manageable
        \item RNN may be more efficient for very long sequences
    \end{itemize}
    
    \textbf{Practical Implications:}
    \begin{itemize}
        \item Self-attention excels with parallel hardware (GPUs)
        \item RNNs better for extremely long sequences
        \item Memory requirements can be limiting factor for self-attention
    \end{itemize}
    }
    
    \item Explain why self-attention is permutation invariant and how positional encoding addresses this limitation. \hfill (4 marks)
    
    \answer{Self-attention treats input as a set, losing order information. Positional encoding injects position-specific signals to restore sequence order awareness.}
    
    \explanation{
    \textbf{Permutation Invariance in Self-Attention:}
    
    \textbf{Why It Occurs:}
    \begin{itemize}
        \item Attention weights depend only on content similarity: $\alpha_{ij} \propto \exp(q_i^T k_j)$
        \item No inherent notion of position in the computation
        \item Swapping positions $i$ and $j$ doesn't change the final representations
        \item Mathematical proof: $f(\pi(X)) = \pi(f(X))$ for any permutation $\pi$
    \end{itemize}
    
    \textbf{Problem Illustration:}
    \begin{itemize}
        \item "Cat chased dog" vs "Dog chased cat"
        \item Both would produce identical embeddings without positional information
        \item Critical semantic differences lost
    \end{itemize}
    
    \textbf{Positional Encoding Solution:}
    
    \textbf{Basic Approach:}
    $$\text{Input} = \text{Token Embedding} + \text{Positional Encoding}$$
    
    \textbf{How It Works:}
    \begin{itemize}
        \item Each position gets unique encoding vector
        \item Combined with content embeddings before attention
        \item Attention now sees both content and position information
        \item Different positions produce different representations
    \end{itemize}
    
    \textbf{Types of Positional Encoding:}
    \begin{itemize}
        \item \textbf{Learned:} Position-specific parameters trained end-to-end
        \item \textbf{Sinusoidal:} Fixed trigonometric functions with different frequencies
        \item \textbf{Relative:} Encoding relative distances between positions
    \end{itemize}
    
    \textbf{Effectiveness:}
    Position encoding breaks permutation invariance while preserving the parallel processing benefits of self-attention.
    }
\end{enumerate>

\newpage
\paragraph{Question 2. Multi-Head Attention Architecture}\hfill (30 marks)\\
Based on "Attention Is All You Need" and related transformer literature.

\begin{enumerate}[(a)]
    \item Design a multi-head attention mechanism with $h=8$ heads for input dimension $d_{model}=512$. Calculate: \hfill (12 marks)
    \begin{itemize}
        \item Dimension of each head: $d_k = d_v = ?$
        \item Total number of parameters in all projection matrices
        \item Memory requirements for storing attention matrices
        \item Computational complexity compared to single-head attention
    \end{itemize}
    
    \answer{Multi-head attention with 8 heads and 512-dimensional embeddings, showing parameter count and complexity analysis.}
    
    \explanation{
    \textbf{Multi-Head Attention Design for $h=8$, $d_{model}=512$:}
    
    \textbf{1. Dimension of Each Head:}
    $$d_k = d_v = \frac{d_{model}}{h} = \frac{512}{8} = 64$$
    
    \textbf{Rationale:} Equal split ensures total dimension remains $d_{model}$ after concatenation.
    
    \textbf{2. Parameter Count Calculation:}
    
    \textbf{Per Head Parameters:}
    \begin{itemize}
        \item $W_Q^{(i)} \in \mathbb{R}^{512 \times 64}$: 32,768 parameters
        \item $W_K^{(i)} \in \mathbb{R}^{512 \times 64}$: 32,768 parameters  
        \item $W_V^{(i)} \in \mathbb{R}^{512 \times 64}$: 32,768 parameters
        \item Total per head: $3 \times 32,768 = 98,304$ parameters
    \end{itemize}
    
    \textbf{All Heads:}
    $8 \times 98,304 = 786,432$ parameters
    
    \textbf{Output Projection:}
    $W_O \in \mathbb{R}^{512 \times 512}$: 262,144 parameters
    
    \textbf{Total Parameters:} $786,432 + 262,144 = 1,048,576$ parameters
    
    \textbf{3. Memory Requirements (for sequence length $n$):}
    
    \textbf{Per Head:}
    \begin{itemize}
        \item Q, K, V matrices: $3 \times n \times 64$ values
        \item Attention matrix: $n \times n$ values
        \item Output: $n \times 64$ values
    \end{itemize}
    
    \textbf{All Heads:}
    \begin{itemize}
        \item QKV storage: $8 \times 3 \times n \times 64 = 1,536n$ values
        \item Attention matrices: $8 \times n^2 = 8n^2$ values
        \item Head outputs: $8 \times n \times 64 = 512n$ values
        \item \textbf{Total: $8n^2 + 2,048n$ values}
    \end{itemize}
    
    \textbf{4. Computational Complexity vs Single-Head:}
    
    \textbf{Single-Head Attention ($d_k = 512$):}
    \begin{itemize}
        \item QKV projections: $O(n \times 512^2) = O(262,144n)$
        \item Attention computation: $O(n^2 \times 512)$
    \end{itemize}
    
    \textbf{Multi-Head Attention (8 heads, $d_k = 64$ each):}
    \begin{itemize}
        \item QKV projections: $O(8 \times n \times 512 \times 64) = O(262,144n)$ (same!)
        \item Attention computation: $O(8 \times n^2 \times 64) = O(512n^2)$ (same!)
        \item Output projection: $O(n \times 512^2) = O(262,144n)$
    \end{itemize}
    
    \textbf{Key Insight:} Multi-head attention has the same computational complexity as single-head attention but provides much richer representations through parallel attention patterns.
    }
    
    \item Implement the multi-head attention algorithm in pseudocode. Include: \hfill (10 marks)
    \begin{itemize}
        \item Input preprocessing
        \item Parallel head computation
        \item Output concatenation and projection
        \item Masking for causal attention
    \end{itemize}
    
    \answer{Complete multi-head attention algorithm with masking support.}
    
    \explanation{
    \textbf{Multi-Head Attention Algorithm:}
    
    \begin{algorithmic}[1]
    \STATE \textbf{function} MultiHeadAttention($X$, $mask=None$)
    \STATE \textbf{Input:} $X \in \mathbb{R}^{n \times d_{model}}$, optional mask
    \STATE \textbf{Output:} $Z \in \mathbb{R}^{n \times d_{model}}$
    \STATE
    \STATE // \textbf{Input Preprocessing}
    \STATE $n, d_{model} \leftarrow \text{shape}(X)$
    \STATE $d_k \leftarrow d_{model} / h$
    \STATE
    \STATE // \textbf{Initialize head outputs list}
    \STATE $\text{head\_outputs} \leftarrow []$
    \STATE
    \STATE // \textbf{Parallel Head Computation}
    \FOR{$i = 1$ to $h$}
        \STATE // Project to Q, K, V for head $i$
        \STATE $Q^{(i)} \leftarrow X \cdot W_Q^{(i)}$ \COMMENT{$\mathbb{R}^{n \times d_k}$}
        \STATE $K^{(i)} \leftarrow X \cdot W_K^{(i)}$ \COMMENT{$\mathbb{R}^{n \times d_k}$}
        \STATE $V^{(i)} \leftarrow X \cdot W_V^{(i)}$ \COMMENT{$\mathbb{R}^{n \times d_k}$}
        \STATE
        \STATE // Compute scaled dot-product attention
        \STATE $\text{scores} \leftarrow \frac{Q^{(i)} \cdot K^{(i)T}}{\sqrt{d_k}}$ \COMMENT{$\mathbb{R}^{n \times n}$}
        \STATE
        \STATE // \textbf{Apply Masking (if provided)}
        \IF{$mask \neq None$}
            \STATE $\text{scores} \leftarrow \text{scores} + \text{mask} \times (-\infty)$
        \ENDIF
        \STATE
        \STATE // Softmax normalization
        \STATE $\text{attn\_weights} \leftarrow \text{softmax}(\text{scores})$ \COMMENT{along last dim}
        \STATE
        \STATE // Weighted aggregation
        \STATE $\text{head\_output} \leftarrow \text{attn\_weights} \cdot V^{(i)}$ \COMMENT{$\mathbb{R}^{n \times d_k}$}
        \STATE $\text{head\_outputs.append}(\text{head\_output})$
    \ENDFOR
    \STATE
    \STATE // \textbf{Output Concatenation and Projection}
    \STATE $\text{concatenated} \leftarrow \text{concat}(\text{head\_outputs})$ \COMMENT{$\mathbb{R}^{n \times d_{model}}$}
    \STATE $Z \leftarrow \text{concatenated} \cdot W_O$ \COMMENT{Final projection}
    \STATE
    \STATE \textbf{return} $Z$
    \end{algorithmic}
    
    \textbf{Masking Types:}
    
    \textbf{1. Causal Mask (for decoder):}
    \begin{itemize}
        \item Upper triangular matrix with $-\infty$ values
        \item Prevents attention to future positions
        \item mask$[i,j] = -\infty$ if $j > i$, else $0$
    \end{itemize}
    
    \textbf{2. Padding Mask:}
    \begin{itemize}
        \item Masks out padding tokens
        \item mask$[i,j] = -\infty$ if position $j$ is padding, else $0$
    \end{itemize}
    
    \textbf{Implementation Notes:}
    \begin{itemize}
        \item Can be vectorized for efficiency
        \item Gradient computation handled automatically by autodiff
        \item Memory optimization: compute attention heads sequentially if memory-constrained
    \end{itemize}
    }
    
    \item Analyze why multiple attention heads capture different types of relationships. Provide examples of what different heads might learn in language modeling. \hfill (8 marks)
    
    \answer{Multiple heads learn specialized attention patterns capturing diverse linguistic relationships through different Q, K, V projections and training dynamics.}
    
    \explanation{
    \textbf{Why Multiple Heads Capture Different Relationships:}
    
    \textbf{1. Different Parameter Initialization:}
    \begin{itemize}
        \item Each head has independent $W_Q^{(i)}$, $W_K^{(i)}$, $W_V^{(i)}$ matrices
        \item Random initialization leads to different gradient flows
        \item Heads evolve to minimize different aspects of the loss
    \end{itemize}
    
    \textbf{2. Representation Subspace Specialization:}
    \begin{itemize}
        \item Each head projects embeddings to different $d_k$-dimensional subspace
        \item Different subspaces can capture orthogonal linguistic features
        \item Example: syntactic vs semantic subspaces
    \end{itemize}
    
    \textbf{3. Training Dynamics and Competition:}
    \begin{itemize}
        \item Heads compete to provide useful signals
        \item Natural specialization emerges to minimize redundancy
        \item Different heads find different optima in parameter space
    \end{itemize}
    
    \textbf{Examples of Learned Attention Patterns:}
    
    \textbf{Head 1 - Syntactic Dependencies:}
    \begin{itemize}
        \item Subject-verb agreement: "The cats [MASK] running"
        \item High attention from "cats" to "[MASK]" for verb prediction
        \item Captures grammatical number agreement
    \end{itemize}
    
    \textbf{Head 2 - Semantic Relationships:}
    \begin{itemize}
        \item Thematic roles: "John ate the [MASK]"
        \item High attention from "ate" to "[MASK]" for object prediction
        \item Captures semantic selectional preferences
    \end{itemize}
    
    \textbf{Head 3 - Coreference Resolution:}
    \begin{itemize}
        \item Pronoun resolution: "The dog saw the cat. It [MASK]"
        \item High attention from "It" to "dog" and "cat"
        \item Learns to track entity references
    \end{itemize}
    
    \textbf{Head 4 - Local Context:}
    \begin{itemize}
        \item Adjacent word relationships: "New [MASK]" → "York"
        \item Strong attention to immediately neighboring positions
        \item Captures local collocations and phrases
    \end{itemize}
    
    \textbf{Head 5 - Long-Range Dependencies:}
    \begin{itemize}
        \item Nested structures: "The man who [clause] [MASK]"
        \item Attention spans across intervening clauses
        \item Maintains memory of distant relevant context
    \end{itemize}
    
    \textbf{Head 6 - Part-of-Speech Patterns:}
    \begin{itemize}
        \item Adjective-noun relationships: "big red [MASK]" → noun
        \item Attention from adjectives to head nouns
        \item Captures morphosyntactic patterns
    \end{itemize}
    
    \textbf{Empirical Evidence:}
    \begin{itemize}
        \item Probing studies show heads learn interpretable patterns
        \item Attention visualization reveals linguistic structure
        \item Head ablation studies demonstrate specialized functions
        \item Different layers show different specialization patterns
    \end{itemize}
    }
\end{enumerate}

\vfill
\begin{center}{\bf END OF PAPER}\end{center>
\end{document}